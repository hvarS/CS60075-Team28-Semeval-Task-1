{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XDM8FtGrv3Cu",
        "fH-qxn1zojp2",
        "Svgv6n9e-Q7I",
        "_1wXzYgmV2Mx",
        "7TIJwE4UsGOg",
        "vYQs4natxevq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hvarS/CS60075-Team28-Task-1/blob/main/baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw13Jr9RdCrS"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZpllGi738qY",
        "outputId": "9923627e-5796-4868-da13-0f7dee2ce731"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkQkKVA35L-3",
        "outputId": "f62abbe7-1516-4d4e-c906-06df6ae06ddb"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d embeddings"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-10 13:09:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-10 13:09:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-10 13:09:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 40s  \n",
            "\n",
            "2021-04-10 13:12:18 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: embeddings/glove.6B.50d.txt  \n",
            "  inflating: embeddings/glove.6B.100d.txt  \n",
            "  inflating: embeddings/glove.6B.200d.txt  \n",
            "  inflating: embeddings/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0IfcPd_6GR4"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr3O0KwxUfLR"
      },
      "source": [
        "FOLDER_PATH = \"/content/drive/MyDrive/CS60075-Team28-Task-1\"\n",
        "DATA_FOLDER = os.path.join(FOLDER_PATH,\"data/preprocessed\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbKLlGrPKfTu"
      },
      "source": [
        "# import evaluate function\n",
        "import sys\n",
        "sys.path.append(FOLDER_PATH)\n",
        "from eval import evaluate"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LeyCMxKPtcOv",
        "outputId": "508c6077-927d-43aa-e3cf-84bc92560374"
      },
      "source": [
        "data = pd.read_csv(os.path.join(DATA_FOLDER, \"lcp_single_train_preprocessed.csv\"), index_col=0)\n",
        "data['token'] = data['token'].astype(str)\n",
        "data['sentence'] = data['sentence'].astype(str)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3ZLW647WALVGE8EBR50EGUBPU4P32A</th>\n",
              "      <td>bible</td>\n",
              "      <td>behold came river seven cattle sleek fat fed m...</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34R0BODSP1ZBN3DVY8J8XSIY551E5C</th>\n",
              "      <td>bible</td>\n",
              "      <td>fellow bondservant brother prophet keep word book</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</th>\n",
              "      <td>bible</td>\n",
              "      <td>man lord land said u know honest men leave one...</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3BFNCI9LYKQN09BHXHH9CLSX5KP738</th>\n",
              "      <td>bible</td>\n",
              "      <td>shimei sixteen son six daughter brother didnt ...</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</th>\n",
              "      <td>bible</td>\n",
              "      <td>put brother far</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.263889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               corpus  ... complexity\n",
              "id                                     ...           \n",
              "3ZLW647WALVGE8EBR50EGUBPU4P32A  bible  ...   0.000000\n",
              "34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible  ...   0.000000\n",
              "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible  ...   0.050000\n",
              "3BFNCI9LYKQN09BHXHH9CLSX5KP738  bible  ...   0.150000\n",
              "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  bible  ...   0.263889\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "yQ_QxnnMp6eV",
        "outputId": "3c0e22fe-2a5a-4324-dad5-5fcabebca4bf"
      },
      "source": [
        "data_multi = pd.read_csv(os.path.join(DATA_FOLDER, \"lcp_multi_train_preprocessed.csv\"), index_col=0)\n",
        "data_multi['token'] = data_multi['token'].astype(str)\n",
        "data_multi['sentence'] = data_multi['sentence'].astype(str)\n",
        "\n",
        "data_multi.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</th>\n",
              "      <td>bible</td>\n",
              "      <td>seventh day sabbath yahweh god shall work son ...</td>\n",
              "      <td>seventh day</td>\n",
              "      <td>0.027778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3WGCNLZJKF877FYC1Q6COKNWTDWD11</th>\n",
              "      <td>bible</td>\n",
              "      <td>let man test own work take pride neighbor</td>\n",
              "      <td>own work</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3UOMW19E6D6WQ5TH2HDD74IVKTP5CB</th>\n",
              "      <td>bible</td>\n",
              "      <td>understanding made heaven loving kindness endu...</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36JW4WBR06KF9AXMUL4N476OMF8FHD</th>\n",
              "      <td>bible</td>\n",
              "      <td>remember god also spare according greatness lo...</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A</th>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness better life lip shall praise</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.075000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               corpus  ... complexity\n",
              "id                                     ...           \n",
              "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  bible  ...   0.027778\n",
              "3WGCNLZJKF877FYC1Q6COKNWTDWD11  bible  ...   0.050000\n",
              "3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  bible  ...   0.050000\n",
              "36JW4WBR06KF9AXMUL4N476OMF8FHD  bible  ...   0.050000\n",
              "3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A  bible  ...   0.075000\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iuUvPq0_4Ob"
      },
      "source": [
        "# # use if model is to be trained on both data together\n",
        "\n",
        "# data = pd.concat([data, data_multi])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shXwG0Yj-VQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcdd3d0-3e56-4c5c-9f93-3f759511d094"
      },
      "source": [
        "def read_glove_vector(glove_vec):\n",
        "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
        "    words = set()\n",
        "    word_to_vec_map = {}\n",
        "    for line in f:\n",
        "      w_line = line.split()\n",
        "      curr_word = w_line[0]\n",
        "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
        "\n",
        "  return word_to_vec_map\n",
        "\n",
        "word_to_vec_map = read_glove_vector('embeddings/glove.6B.300d.txt')\n",
        "print(len(word_to_vec_map),\" words loaded!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000  words loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwutHald-WLn"
      },
      "source": [
        "def get_embeddings(sentences, tokens):\n",
        "    token_emb = []\n",
        "    for s,t in zip(sentences, tokens):\n",
        "        \n",
        "        # fill unk by nan\n",
        "        # calculate mean over non nan embeddings\n",
        "        # fill unk by the mean embedding of sentence\n",
        "        # pad 0 vectors till max_len\n",
        "        \n",
        "        temp_emb = [ word_to_vec_map[x] if x in word_to_vec_map else np.full((300,), np.nan) for x in t.split() ]\n",
        "        \n",
        "        # calculate mean for filling null values <unk>\n",
        "        temp_sent_emb = [ word_to_vec_map[x] if x in word_to_vec_map else np.full((300,), np.nan) for x in s.split() ]\n",
        "        mean_emb = np.nanmean(np.array(temp_sent_emb), axis=0)\n",
        "        \n",
        "        # single or multi - will be converted to (1,300) \n",
        "        temp_emb = np.mean(np.array([ mean_emb if np.isnan(x[0]) else x for x in temp_emb ]), axis=0)\n",
        "\n",
        "        token_emb.append(temp_emb)\n",
        "\n",
        "    return np.array(token_emb)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDM8FtGrv3Cu"
      },
      "source": [
        "## CNN Regression Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUc4c5H8wWiz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split             \n",
        "from keras.preprocessing.text import Tokenizer                    \n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEOeEAEfwAJj"
      },
      "source": [
        "sentences_train_list = list(data['sentence'])\n",
        "complexity_train_list = list(data['complexity'])\n",
        "\n",
        "sentences_train,sentences_test,y_train,y_test = train_test_split(\n",
        "                                                sentences_train_list, complexity_train_list,  \n",
        "                                                test_size=0.25,  \n",
        "                                                random_state=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHsiKksPwilG"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "# Adding 1 because of  reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1                          \n",
        "\n",
        "maxlen = 128\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zo9sbagw1vW"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "  vocab_size = len(word_index) + 1  \n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word in word_index:\n",
        "        idx = word_index[word] \n",
        "        embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf3JmHZHxSgc"
      },
      "source": [
        "embedding_dim = 300\n",
        "embedding_matrix = create_embedding_matrix('embeddings/glove.6B.300d.txt ,tokenizer.word_index,  \n",
        "                                            embedding_dim' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH-qxn1zojp2"
      },
      "source": [
        "## **Test Single Word**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJzy1Juz_gqx"
      },
      "source": [
        "SUBMISSION_FOLDER = os.path.join(FOLDER_PATH,\"predictions/baselines_ankit/single\")\n",
        "\n",
        "if( not os.path.exists(SUBMISSION_FOLDER)):\n",
        "    os.makedirs(SUBMISSION_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3iXv8sIXSyR",
        "outputId": "35bbc659-c9ab-4195-a35b-3d4665e7b1d3"
      },
      "source": [
        "sentences_train_list = list(data['sentence'])\n",
        "complexity_train_list = list(data['complexity'])\n",
        "tokens_train_list = list(data['token'])\n",
        "\n",
        "vectors = get_embeddings(sentences_train_list, tokens_train_list)\n",
        "print(vectors.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9179, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXS52dlZhyXt"
      },
      "source": [
        "test_df = pd.read_csv(os.path.join(DATA_FOLDER, \"lcp_single_test_preprocessed.csv\"), index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3FLUZ7NdmHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087b8f43-9b45-47dc-8ad0-be5afbbf5ba5"
      },
      "source": [
        "test_df['token'] = test_df['token'].astype(str)\n",
        "test_df['sentence'] = test_df['sentence'].astype(str)\n",
        "sentences_test_list = list(test_df['sentence'])\n",
        "test_tokens_list = list(test_df['token'])\n",
        "\n",
        "testdf_vectors = get_embeddings(sentences_test_list, test_tokens_list)\n",
        "testdf_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(917, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHiacxNifhJ"
      },
      "source": [
        "# Linear Regression\n",
        "reg = LinearRegression().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/linear_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# Gradient Boosting\n",
        "reg = GradientBoostingRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/gradient_boosting_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# AdaBoost\n",
        "reg = AdaBoostRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/ada_boost_baseline.csv\", index=False, header=False)\n",
        "\n",
        "\n",
        "# SVM regressor\n",
        "reg = SVR().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/SVM_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# MLP Regressor\n",
        "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6VHoYyIin98",
        "outputId": "09d596c4-3e71-4a2b-e7d8-6c9576218336"
      },
      "source": [
        "evaluate(SUBMISSION_FOLDER, FOLDER_PATH+\"/references/lcp_single_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file linear_regression_baseline.csv\n",
            "pearson  :  0.6715029466644609\n",
            "spearman :  0.6548482413646795\n",
            "mae      :  0.0759823204469714\n",
            "mse      :  0.009499099203375246\n",
            "r2       :  0.4130951539079504\n",
            "\n",
            "For file gradient_boosting_baseline.csv\n",
            "pearson  :  0.6939712352149733\n",
            "spearman :  0.6731328271116146\n",
            "mae      :  0.0715689474868356\n",
            "mse      :  0.008668781235018004\n",
            "r2       :  0.46439661197178606\n",
            "\n",
            "For file ada_boost_baseline.csv\n",
            "pearson  :  0.6631336571796327\n",
            "spearman :  0.6357005159629594\n",
            "mae      :  0.08355461007871447\n",
            "mse      :  0.011214615888546787\n",
            "r2       :  0.30710141339398755\n",
            "\n",
            "For file SVM_baseline.csv\n",
            "pearson  :  0.6678429730298119\n",
            "spearman :  0.6568058055125834\n",
            "mae      :  0.07496900141336407\n",
            "mse      :  0.009234749134759981\n",
            "r2       :  0.42942810643464235\n",
            "\n",
            "For file MLP_baseline.csv\n",
            "pearson  :  0.5207687017064077\n",
            "spearman :  0.5172088187127104\n",
            "mae      :  0.10987027960820457\n",
            "mse      :  0.02178758707478297\n",
            "r2       :  -0.3461529525135425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcbIxJtE_wrW"
      },
      "source": [
        "For file gradient_boosting_baseline.csv\n",
        "pearson  :  0.7214444729661804\n",
        "spearman :  0.6954546622318855\n",
        "mae      :  0.06718193713274725\n",
        "mse      :  0.00777120977316972\n",
        "r2       :  0.5198533483837506"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svgv6n9e-Q7I"
      },
      "source": [
        "## **Multi Word**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Z8spmk_cCb"
      },
      "source": [
        "SUBMISSION_FOLDER = os.path.join(FOLDER_PATH,\"predictions/baselines_ankit/multi\")\n",
        "\n",
        "if( not os.path.exists(SUBMISSION_FOLDER)):\n",
        "    os.makedirs(SUBMISSION_FOLDER)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQDHP9vr-jlz"
      },
      "source": [
        "#  DO NOT run if both is being trained together\n",
        "sentences_train_list = list(data_multi['sentence'])\n",
        "complexity_train_list = list(data_multi['complexity'])\n",
        "tokens_train_list = list(data_multi['token'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccLbHMyM-jl1",
        "outputId": "889e0cb2-1efd-452a-f756-acebfca756c8"
      },
      "source": [
        "#  DO NOT run if both is being trained together\n",
        "vectors = get_embeddings(sentences_train_list, tokens_train_list)\n",
        "print(vectors.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1517, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYtjY-_T9VNG"
      },
      "source": [
        "test_df = pd.read_csv(os.path.join(DATA_FOLDER, \"lcp_multi_test_preprocessed.csv\"), index_col=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2UTW87W9VNG"
      },
      "source": [
        "test_df['token'] = test_df['token'].astype(str)\n",
        "test_df['sentence'] = test_df['sentence'].astype(str)\n",
        "sentences_test_list = list(test_df['sentence'])\n",
        "test_tokens_list = list(test_df['token'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiM-EnF79VNH",
        "outputId": "5381a7b5-a232-4d29-92a7-ac45c34bf554"
      },
      "source": [
        "testdf_vectors = get_embeddings(sentences_test_list, test_tokens_list)\n",
        "testdf_vectors.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(184, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv8DXM5X9VNH"
      },
      "source": [
        "# Linear Regression\n",
        "reg = LinearRegression().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/linear_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# Gradient Boosting\n",
        "reg = GradientBoostingRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/gradient_boosting_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# AdaBoost\n",
        "reg = AdaBoostRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/ada_boost_baseline.csv\", index=False, header=False)\n",
        "\n",
        "\n",
        "# SVM regressor\n",
        "reg = SVR().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/SVM_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# MLP Regressor\n",
        "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(testdf_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvkkKdbS9VNJ",
        "outputId": "17382fc5-09ab-49a2-8a1a-ae106597b39a"
      },
      "source": [
        "evaluate(SUBMISSION_FOLDER, FOLDER_PATH+\"/references/lcp_multi_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file linear_regression_baseline.csv\n",
            "pearson  :  0.8179201155507047\n",
            "spearman :  0.8049744407871943\n",
            "mae      :  0.10626544460880245\n",
            "mse      :  0.0162353528148035\n",
            "r2       :  0.3273702640454511\n",
            "\n",
            "For file gradient_boosting_baseline.csv\n",
            "pearson  :  0.8194393024562177\n",
            "spearman :  0.8088855582364516\n",
            "mae      :  0.10527705497011969\n",
            "mse      :  0.016060012182902487\n",
            "r2       :  0.3346346163686199\n",
            "\n",
            "For file ada_boost_baseline.csv\n",
            "pearson  :  0.7681339800348336\n",
            "spearman :  0.7520875670765769\n",
            "mae      :  0.10133574216425925\n",
            "mse      :  0.01539232239492464\n",
            "r2       :  0.3622969659898482\n",
            "\n",
            "For file SVM_baseline.csv\n",
            "pearson  :  0.8137261535536578\n",
            "spearman :  0.8074492340697415\n",
            "mae      :  0.07244008552665147\n",
            "mse      :  0.008700322647294615\n",
            "r2       :  0.6395461317210687\n",
            "\n",
            "For file MLP_baseline.csv\n",
            "pearson  :  0.645835030234091\n",
            "spearman :  0.6058812226904214\n",
            "mae      :  0.10435937152315199\n",
            "mse      :  0.016977828837603102\n",
            "r2       :  0.2966095250048487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnBDIaZS08B_"
      },
      "source": [
        "For file linear_regression_baseline.csv\n",
        "pearson  :  0.7384735689252496\n",
        "spearman :  0.7096751511559939\n",
        "mae      :  0.08420414291697727\n",
        "mse      :  0.011177467719053704\n",
        "r2       :  0.5369181534723191\n",
        "\n",
        "For file gradient_boosting_baseline.csv\n",
        "pearson  :  0.7870767227623038\n",
        "spearman :  0.7664566772179571\n",
        "mae      :  0.07831874126395687\n",
        "mse      :  0.009391251928283358\n",
        "r2       :  0.6109209712372773\n",
        "\n",
        "For file SVM_baseline.csv\n",
        "pearson  :  0.7747686594582374\n",
        "spearman :  0.7563503882622532\n",
        "mae      :  0.07906457106883466\n",
        "mse      :  0.009776174954316456\n",
        "r2       :  0.5949736323456092"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCS9A2TN1IcT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1wXzYgmV2Mx"
      },
      "source": [
        "## Features - load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmAPF6kVbdX8",
        "outputId": "f212416d-04c9-4e0a-9ebd-94cf49d35543"
      },
      "source": [
        "f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_single_train_features.csv\"), index_col=0)\n",
        "f1['token'] = f1['token'].astype(str)\n",
        "f1['sentence'] = f1['sentence'].astype(str)\n",
        "f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "f1.drop(['parse', 'lemma'], axis=1, inplace=True)\n",
        "\n",
        "print(f1.columns)\n",
        "\n",
        "f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_single_train_preprocessed.csv\"), index_col=0)\n",
        "f2['token'] = f2['token'].astype(str)\n",
        "f2['sentence'] = f2['sentence'].astype(str)\n",
        "print(f2.columns)\n",
        "\n",
        "features = f1.merge(f2, on=['id','sentence', 'corpus', 'token', 'complexity'])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['sentence', 'corpus', 'token', 'complexity', 'token_length',\n",
            "       'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms',\n",
            "       'google frequency'],\n",
            "      dtype='object')\n",
            "Index(['corpus', 'sentence', 'token', 'complexity', 'biomedical', 'bible',\n",
            "       'subtitles', 'wiki', 'familarity'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "-G3zFmbnhPqB",
        "outputId": "c5a10a5d-601a-412a-db7e-b58611c36890"
      },
      "source": [
        "# fill pos nan by NN, as they are in majority\n",
        "features['pos'] = features['pos'].fillna('NN')\n",
        "features['token_length'] = features['token_length'].fillna(0)\n",
        "\n",
        "# categorical encoding\n",
        "labels = dict(features['pos'].value_counts())\n",
        "labels = { k:i for i,k in enumerate(labels)}\n",
        "labels['POS'] = len(labels)\n",
        "print(labels)\n",
        "\n",
        "\n",
        "def get_vowels(word):\n",
        "    val = 0\n",
        "    for w in word:\n",
        "        if(w in ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U','u']):\n",
        "            val+=1\n",
        "    return val\n",
        "\n",
        "features['token_vowels'] = features['token'].apply(lambda x: get_vowels(x) )\n",
        "\n",
        "\n",
        "features['pos'] = features['pos'].apply((lambda x: labels[x]))\n",
        "\n",
        "# scaler = preprocessing.StandardScaler()\n",
        "# features[['token_length', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'familarity', 'token_vowels']] =  \\\n",
        "#     scaler.fit_transform(features[['token_length', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'familarity', 'token_vowels']])\n",
        "\n",
        "\n",
        "features.head()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'NN': 0, 'JJ': 1, 'NNP': 2, 'NNS': 3, 'VBG': 4, 'VB': 5, 'RB': 6, 'VBP': 7, 'CD': 8, 'UH': 9, 'VBN': 10, 'FW': 11, 'VBZ': 12, 'IN': 13, 'VBD': 14, 'JJR': 15, 'PRP': 16, 'GW': 17, 'WRB': 18, 'LS': 19, 'AFX': 20, 'MD': 21, 'JJS': 22, 'POS': 23}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos</th>\n",
              "      <th>dep num</th>\n",
              "      <th>synonyms</th>\n",
              "      <th>hypernyms</th>\n",
              "      <th>hyponyms</th>\n",
              "      <th>google frequency</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "      <th>token_vowels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3ZLW647WALVGE8EBR50EGUBPU4P32A</th>\n",
              "      <td>behold came river seven cattle sleek fat fed m...</td>\n",
              "      <td>bible</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>173.485953</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>565</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34R0BODSP1ZBN3DVY8J8XSIY551E5C</th>\n",
              "      <td>fellow bondservant brother prophet keep word book</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</th>\n",
              "      <td>man lord land said u know honest men leave one...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3BFNCI9LYKQN09BHXHH9CLSX5KP738</th>\n",
              "      <td>shimei sixteen son six daughter brother didnt ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</th>\n",
              "      <td>put brother far</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... token_vowels\n",
              "id                                                                                 ...             \n",
              "3ZLW647WALVGE8EBR50EGUBPU4P32A  behold came river seven cattle sleek fat fed m...  ...            2\n",
              "34R0BODSP1ZBN3DVY8J8XSIY551E5C  fellow bondservant brother prophet keep word book  ...            2\n",
              "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  man lord land said u know honest men leave one...  ...            2\n",
              "3BFNCI9LYKQN09BHXHH9CLSX5KP738  shimei sixteen son six daughter brother didnt ...  ...            2\n",
              "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2                                    put brother far  ...            2\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ocBaxNJ1uPD-",
        "outputId": "2e9d04ce-f680-48b5-87c6-56e406ae831c"
      },
      "source": [
        "multi_f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_multi_train_split_features.csv\"), index_col=0)\n",
        "multi_f1['token'] = multi_f1['token'].astype(str)\n",
        "multi_f1['sentence'] = multi_f1['sentence'].astype(str)\n",
        "multi_f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "multi_f1.drop(['parse', 'token1', 'token2', 'lemma1', 'lemma2', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
        "\n",
        "multi_f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_multi_train_preprocessed.csv\"), index_col=0)\n",
        "multi_f2['token'] = multi_f2['token'].astype(str)\n",
        "multi_f2['sentence'] = multi_f2['sentence'].astype(str)\n",
        "\n",
        "multi_features = multi_f1.merge(multi_f2, on=['id','sentence', 'corpus', 'token', 'complexity'])\n",
        "multi_features.head(2)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</th>\n",
              "      <td>seventh day sabbath yahweh god shall work son ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>seventh day</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>JJ</td>\n",
              "      <td>0</td>\n",
              "      <td>NN</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>24.522564</td>\n",
              "      <td>682.298213</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>297.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3WGCNLZJKF877FYC1Q6COKNWTDWD11</th>\n",
              "      <td>let man test own work take pride neighbor</td>\n",
              "      <td>bible</td>\n",
              "      <td>own work</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>JJ</td>\n",
              "      <td>0</td>\n",
              "      <td>NN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1022.711588</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>600.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  seventh day sabbath yahweh god shall work son ...  ...      297.5\n",
              "3WGCNLZJKF877FYC1Q6COKNWTDWD11          let man test own work take pride neighbor  ...      600.5\n",
              "\n",
              "[2 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "6oRx6fW5uxx1",
        "outputId": "a2b50257-da5f-4f30-897b-87dd82ae6bdc"
      },
      "source": [
        "# fill pos nan by NN, as they are in majority\n",
        "multi_features['pos2'] = multi_features['pos2'].fillna('NN')\n",
        "\n",
        "multi_features['pos1'] = multi_features['pos1'].apply((lambda x: labels[x]))\n",
        "multi_features['pos2'] = multi_features['pos2'].apply((lambda x: labels[x]))\n",
        "\n",
        "# scaler = preprocessing.StandardScaler()\n",
        "# multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "#     scaler.fit_transform(multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "\n",
        "multi_features.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</th>\n",
              "      <td>seventh day sabbath yahweh god shall work son ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>seventh day</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>24.522564</td>\n",
              "      <td>682.298213</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>297.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3WGCNLZJKF877FYC1Q6COKNWTDWD11</th>\n",
              "      <td>let man test own work take pride neighbor</td>\n",
              "      <td>bible</td>\n",
              "      <td>own work</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1022.711588</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>600.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3UOMW19E6D6WQ5TH2HDD74IVKTP5CB</th>\n",
              "      <td>understanding made heaven loving kindness endu...</td>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>339.132903</td>\n",
              "      <td>14.221491</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>566.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36JW4WBR06KF9AXMUL4N476OMF8FHD</th>\n",
              "      <td>remember god also spare according greatness lo...</td>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>339.132903</td>\n",
              "      <td>14.221491</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>566.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A</th>\n",
              "      <td>loving kindness better life lip shall praise</td>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>339.132903</td>\n",
              "      <td>14.221491</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>566.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  seventh day sabbath yahweh god shall work son ...  ...      297.5\n",
              "3WGCNLZJKF877FYC1Q6COKNWTDWD11          let man test own work take pride neighbor  ...      600.5\n",
              "3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  understanding made heaven loving kindness endu...  ...      566.5\n",
              "36JW4WBR06KF9AXMUL4N476OMF8FHD  remember god also spare according greatness lo...  ...      566.5\n",
              "3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A       loving kindness better life lip shall praise  ...      566.5\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "-rS4FNqV5vp1",
        "outputId": "e51af03f-2af6-41cb-e774-b7b196076fde"
      },
      "source": [
        "# merge both single and multi features\n",
        "\n",
        "features['pos1'] = features['pos'].copy()\n",
        "features['pos2'] = features['pos'] \n",
        "\n",
        "features['dep num1'] = features['dep num'] \n",
        "features['dep num2'] = features['dep num'] \n",
        "\n",
        "features['synonyms1'] = features['synonyms'] \n",
        "features['synonyms2'] = features['synonyms'] \n",
        "\n",
        "features['hypernyms1'] = features['hypernyms'] \n",
        "features['hypernyms2'] = features['hypernyms'] \n",
        "\n",
        "features['hyponyms1'] = features['hyponyms'] \n",
        "features['hyponyms2'] = features['hyponyms'] \n",
        "\n",
        "features['google frequency1'] = features['google frequency'] \n",
        "features['google frequency2'] = features['google frequency1'] \n",
        "\n",
        "features.drop(['pos','dep num', 'synonyms', 'hyponyms', 'hypernyms', 'google frequency'], axis=1, inplace=True)\n",
        "\n",
        "features = features.append( multi_features)\n",
        "print(len(features))\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "    scaler.fit_transform(features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "features.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>pos1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3ZLW647WALVGE8EBR50EGUBPU4P32A</th>\n",
              "      <td>behold came river seven cattle sleek fat fed m...</td>\n",
              "      <td>bible</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.797042</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.276865</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.831106</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>-0.369612</td>\n",
              "      <td>0.419993</td>\n",
              "      <td>0.466245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34R0BODSP1ZBN3DVY8J8XSIY551E5C</th>\n",
              "      <td>fellow bondservant brother prophet keep word book</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>1.709990</td>\n",
              "      <td>1.365895</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</th>\n",
              "      <td>man lord land said u know honest men leave one...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>0.098326</td>\n",
              "      <td>-0.098772</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3BFNCI9LYKQN09BHXHH9CLSX5KP738</th>\n",
              "      <td>shimei sixteen son six daughter brother didnt ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>2.515822</td>\n",
              "      <td>2.098228</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</th>\n",
              "      <td>put brother far</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>0.904158</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... google frequency2\n",
              "id                                                                                 ...                  \n",
              "3ZLW647WALVGE8EBR50EGUBPU4P32A  behold came river seven cattle sleek fat fed m...  ...          0.466245\n",
              "34R0BODSP1ZBN3DVY8J8XSIY551E5C  fellow bondservant brother prophet keep word book  ...          0.097424\n",
              "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  man lord land said u know honest men leave one...  ...          0.097424\n",
              "3BFNCI9LYKQN09BHXHH9CLSX5KP738  shimei sixteen son six daughter brother didnt ...  ...          0.097424\n",
              "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2                                    put brother far  ...          0.097424\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TIJwE4UsGOg"
      },
      "source": [
        "## Test with features - single"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLEjoKSBwkS0",
        "outputId": "d5b8f067-62b6-49a4-f6b0-88b71e79ee97"
      },
      "source": [
        "sentences_train_list = list(features['sentence'])\n",
        "complexity_train_list = list(features['complexity'])\n",
        "tokens_train_list = list(features['token'])\n",
        "\n",
        "vectors = get_embeddings(sentences_train_list, tokens_train_list)\n",
        "print(vectors.shape)\n",
        "\n",
        "# f_vectors = features[['token_length', 'token_vowels', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "# or \n",
        "\n",
        "f_vectors = features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "\n",
        "print(f_vectors.shape)\n",
        "vectors = np.concatenate((vectors, f_vectors), axis=1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9179, 300)\n",
            "(9179, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38GMvczOV4ES"
      },
      "source": [
        "SUBMISSION_FOLDER = os.path.join(FOLDER_PATH,\"predictions/baselines_ankit/features/single\")\n",
        "\n",
        "if( not os.path.exists(SUBMISSION_FOLDER)):\n",
        "    os.makedirs(SUBMISSION_FOLDER)"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "_LPMEPl1ioxd",
        "outputId": "aa959aa7-0f25-47d5-821a-09b9ea438e2a"
      },
      "source": [
        "test_f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_single_test_features.csv\"), index_col=0)\n",
        "test_f1['token'] = test_f1['token'].astype(str)\n",
        "test_f1['sentence'] = test_f1['sentence'].astype(str)\n",
        "test_f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "test_f1.drop(['parse', 'lemma'], axis=1, inplace=True)\n",
        "\n",
        "test_f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_single_test_preprocessed.csv\"), index_col=0)\n",
        "test_f2['token'] = test_f2['token'].astype(str)\n",
        "test_f2['sentence'] = test_f2['sentence'].astype(str)\n",
        "\n",
        "test_features = test_f1.merge(test_f2, on=['id','sentence', 'corpus', 'token'])\n",
        "\n",
        "test_features.head()"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos</th>\n",
              "      <th>dep num</th>\n",
              "      <th>synonyms</th>\n",
              "      <th>hypernyms</th>\n",
              "      <th>hyponyms</th>\n",
              "      <th>google frequency</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39TX062QX1OHFOH8FUL76K5L7D3X3S</th>\n",
              "      <td>speak much prince world come nothing</td>\n",
              "      <td>bible</td>\n",
              "      <td>prince</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>57.555460</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3CIS7GGG65JS8I3AZ9RG54AE4MUUEA</th>\n",
              "      <td>house shall turned others field wife together ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>inhabitant</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>NN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1.907882</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379OL9DBSSESUVWY1Z8JGBFG9BTY92</th>\n",
              "      <td>stranger terrible nation cut left mountain val...</td>\n",
              "      <td>bible</td>\n",
              "      <td>bough</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NN</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.513619</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3DFYDSXB2W00JYP2DA272KN69UQUJV</th>\n",
              "      <td>sharpen tongue like sword aim arrow deadly word</td>\n",
              "      <td>bible</td>\n",
              "      <td>arrow</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>NN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>22.707380</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31YWE12TE0CZG7IVH6OXJ1H1CFPX7X</th>\n",
              "      <td>obey leader submit watch behalf soul give acco...</td>\n",
              "      <td>bible</td>\n",
              "      <td>account</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>NN</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>216.439551</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "39TX062QX1OHFOH8FUL76K5L7D3X3S               speak much prince world come nothing  ...        506\n",
              "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA  house shall turned others field wife together ...  ...        469\n",
              "379OL9DBSSESUVWY1Z8JGBFG9BTY92  stranger terrible nation cut left mountain val...  ...          0\n",
              "3DFYDSXB2W00JYP2DA272KN69UQUJV    sharpen tongue like sword aim arrow deadly word  ...        490\n",
              "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X  obey leader submit watch behalf soul give acco...  ...        523\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTjX8NBioyV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "182ab7fa-f2ad-47dc-a941-712d2bac933a"
      },
      "source": [
        "# # fill pos nan by NN, as they are in majority\n",
        "# test_features['pos'] = test_features['pos'].fillna('NN')\n",
        "\n",
        "# # categorical encoding\n",
        "# test_features['pos'] = test_features['pos'].apply((lambda x: labels[x]))\n",
        "\n",
        "# def get_vowels(word):\n",
        "#     val = 0\n",
        "#     for w in word:\n",
        "#         if(w in ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U','u']):\n",
        "#             val+=1\n",
        "#     return val\n",
        "\n",
        "# test_features['token_vowels'] = test_features['token'].apply(lambda x: get_vowels(x) )\n",
        "\n",
        "# test_features[['token_length', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'familarity', 'token_vowels']] =  \\\n",
        "#     scaler.transform(test_features[['token_length', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'familarity', 'token_vowels']])\n",
        "\n",
        "# ##########  for all features\n",
        "\n",
        "# fill pos nan by NN, as they are in majority\n",
        "test_features['pos'] = test_features['pos'].fillna('NN')\n",
        "test_features['token_length'] = test_features['token_length'].fillna(0)\n",
        "\n",
        "test_features['pos'] = test_features['pos'].apply((lambda x: labels[x]))\n",
        "\n",
        "def get_vowels(word):\n",
        "    val = 0\n",
        "    for w in word:\n",
        "        if(w in ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U','u']):\n",
        "            val+=1\n",
        "    return val\n",
        "\n",
        "test_features['token_vowels'] = test_features['token'].apply(lambda x: get_vowels(x) )\n",
        "\n",
        "test_features['pos1'] = test_features['pos'].copy()\n",
        "test_features['pos2'] = test_features['pos'] \n",
        "\n",
        "test_features['dep num1'] = test_features['dep num'] \n",
        "test_features['dep num2'] = test_features['dep num'] \n",
        "\n",
        "test_features['synonyms1'] = test_features['synonyms'] \n",
        "test_features['synonyms2'] = test_features['synonyms'] \n",
        "\n",
        "test_features['hypernyms1'] = test_features['hypernyms'] \n",
        "test_features['hypernyms2'] = test_features['hypernyms'] \n",
        "\n",
        "test_features['hyponyms1'] = test_features['hyponyms'] \n",
        "test_features['hyponyms2'] = test_features['hyponyms'] \n",
        "\n",
        "test_features['google frequency1'] = test_features['google frequency'] \n",
        "test_features['google frequency2'] = test_features['google frequency'] \n",
        "\n",
        "test_features.drop(['pos','dep num', 'synonyms', 'hyponyms', 'hypernyms', 'google frequency'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "test_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "    scaler.transform(test_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "test_features.head()"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>pos1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39TX062QX1OHFOH8FUL76K5L7D3X3S</th>\n",
              "      <td>speak much prince world come nothing</td>\n",
              "      <td>bible</td>\n",
              "      <td>prince</td>\n",
              "      <td>-0.567050</td>\n",
              "      <td>-1.105235</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.047374</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.323568</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.831106</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>0.217289</td>\n",
              "      <td>0.157118</td>\n",
              "      <td>-0.207734</td>\n",
              "      <td>-0.231416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3CIS7GGG65JS8I3AZ9RG54AE4MUUEA</th>\n",
              "      <td>house shall turned others field wife together ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>inhabitant</td>\n",
              "      <td>0.352919</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.903456</td>\n",
              "      <td>0.393219</td>\n",
              "      <td>-0.323568</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.831106</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>1.318919</td>\n",
              "      <td>1.210578</td>\n",
              "      <td>-0.509048</td>\n",
              "      <td>-0.566299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379OL9DBSSESUVWY1Z8JGBFG9BTY92</th>\n",
              "      <td>stranger terrible nation cut left mountain val...</td>\n",
              "      <td>bible</td>\n",
              "      <td>bough</td>\n",
              "      <td>-0.797042</td>\n",
              "      <td>-1.105235</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.920806</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.323568</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>0.904158</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>-0.369612</td>\n",
              "      <td>-0.511183</td>\n",
              "      <td>-0.568672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3DFYDSXB2W00JYP2DA272KN69UQUJV</th>\n",
              "      <td>sharpen tongue like sword aim arrow deadly word</td>\n",
              "      <td>bible</td>\n",
              "      <td>arrow</td>\n",
              "      <td>-0.797042</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.985139</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.323568</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.831106</td>\n",
              "      <td>-0.553188</td>\n",
              "      <td>-0.582030</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>-0.369612</td>\n",
              "      <td>-0.396426</td>\n",
              "      <td>-0.441129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31YWE12TE0CZG7IVH6OXJ1H1CFPX7X</th>\n",
              "      <td>obey leader submit watch behalf soul give acco...</td>\n",
              "      <td>bible</td>\n",
              "      <td>account</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.113499</td>\n",
              "      <td>-0.148320</td>\n",
              "      <td>-0.323568</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.831106</td>\n",
              "      <td>1.561352</td>\n",
              "      <td>1.491500</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>0.052045</td>\n",
              "      <td>-0.000901</td>\n",
              "      <td>0.652574</td>\n",
              "      <td>0.724737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... google frequency2\n",
              "id                                                                                 ...                  \n",
              "39TX062QX1OHFOH8FUL76K5L7D3X3S               speak much prince world come nothing  ...         -0.231416\n",
              "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA  house shall turned others field wife together ...  ...         -0.566299\n",
              "379OL9DBSSESUVWY1Z8JGBFG9BTY92  stranger terrible nation cut left mountain val...  ...         -0.568672\n",
              "3DFYDSXB2W00JYP2DA272KN69UQUJV    sharpen tongue like sword aim arrow deadly word  ...         -0.441129\n",
              "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X  obey leader submit watch behalf soul give acco...  ...          0.724737\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RV33imHjy2j",
        "outputId": "7c48f35d-24ef-4581-f6b1-9bba9d73d032"
      },
      "source": [
        "sentences_test_list = list(test_features['sentence'])\n",
        "test_tokens_list = list(test_features['token'])\n",
        "\n",
        "test_vectors = get_embeddings(sentences_test_list, test_tokens_list)\n",
        "print(test_vectors.shape)\n",
        "\n",
        "# test_f_vectors = test_features[['token_length', 'token_vowels', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "\n",
        "test_f_vectors = test_features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "\n",
        "test_vectors = np.concatenate((test_vectors, test_f_vectors), axis=1)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(917, 300)\n",
            "(917, 320)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvPffbjPjIBi"
      },
      "source": [
        "# # # Gradient Boosting\n",
        "reg = GradientBoostingRegressor(n_estimators=250).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/gradient_boosting_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# Linear Regression\n",
        "reg = LinearRegression().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/linear_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# # xgb Regression\n",
        "# from xgboost import XGBRegressor\n",
        "# reg = XGBRegressor(objective ='reg:squarederror', n_estimators=250).fit(vectors, np.array(complexity_train_list))\n",
        "# y_pred = reg.predict(test_vectors)\n",
        "\n",
        "# pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "# pred.to_csv(SUBMISSION_FOLDER+\"/xgb_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "\n",
        "# # AdaBoost\n",
        "reg = AdaBoostRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/ada_boost_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# SVM regressor\n",
        "reg = SVR().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/SVM_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# MLP Regressor\n",
        "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzQ_4UNLjVJB",
        "outputId": "24c58eeb-a0a9-4820-b83a-3260a2a74023"
      },
      "source": [
        "evaluate(SUBMISSION_FOLDER, FOLDER_PATH+\"/references/lcp_single_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file linear_regression_baseline.csv\n",
            "pearson  :  0.6873239301467149\n",
            "spearman :  0.6756124918065293\n",
            "mae      :  0.07243171984448264\n",
            "mse      :  0.008765169664813761\n",
            "r2       :  0.4584412223771357\n",
            "\n",
            "For file gradient_boosting_baseline.csv\n",
            "pearson  :  0.7378533824786239\n",
            "spearman :  0.6999168323436749\n",
            "mae      :  0.06664044713027792\n",
            "mse      :  0.0074130645274606615\n",
            "r2       :  0.541981465567408\n",
            "\n",
            "For file ada_boost_baseline.csv\n",
            "pearson  :  0.7027358640900995\n",
            "spearman :  0.6865685979721601\n",
            "mae      :  0.0718557095563951\n",
            "mse      :  0.008772848338434495\n",
            "r2       :  0.45796679310094\n",
            "\n",
            "For file SVM_baseline.csv\n",
            "pearson  :  0.7038973712995336\n",
            "spearman :  0.6824395944866258\n",
            "mae      :  0.07072803737728771\n",
            "mse      :  0.008288147574278556\n",
            "r2       :  0.48791418298466294\n",
            "\n",
            "For file MLP_baseline.csv\n",
            "pearson  :  0.4673315964887351\n",
            "spearman :  0.4500192765157349\n",
            "mae      :  0.107402552137898\n",
            "mse      :  0.019513040277462257\n",
            "r2       :  -0.20561935986126856\n",
            "\n",
            "For file xgb_regression_baseline.csv\n",
            "pearson  :  0.7401818660969525\n",
            "spearman :  0.7073212533102632\n",
            "mae      :  0.06548487241928547\n",
            "mse      :  0.007338576601237726\n",
            "r2       :  0.5465837256280308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_RqzVW85_Hd"
      },
      "source": [
        "pearson  :  0.7420614648980268\n",
        "spearman :  0.7104726955580672\n",
        "mae      :  0.06555679719442302\n",
        "mse      :  0.007306746999771031\n",
        "r2       :  0.5485503276131269\n",
        "\n",
        "pearson  :  0.7401818712131418\n",
        "spearman :  0.7073212533102632\n",
        "mae      :  0.06548487250652647\n",
        "mse      :  0.007338576693601775\n",
        "r2       :  0.5465837199212888"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYQs4natxevq"
      },
      "source": [
        "## Test with features - multi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9LuuTs3xevr",
        "outputId": "c9abfa98-2078-451e-f0df-25d99f20b9ed"
      },
      "source": [
        "sentences_train_list = list(multi_features['sentence'])\n",
        "complexity_train_list = list(multi_features['complexity'])\n",
        "tokens_train_list = list(multi_features['token'])\n",
        "\n",
        "vectors = get_embeddings(sentences_train_list, tokens_train_list)\n",
        "print(vectors.shape)\n",
        "\n",
        "f_vectors = multi_features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "print(f_vectors.shape)\n",
        "vectors = np.concatenate((vectors, f_vectors), axis=1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1517, 300)\n",
            "(1517, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTgM0RNsxevu"
      },
      "source": [
        "SUBMISSION_FOLDER = os.path.join(FOLDER_PATH,\"predictions/baselines_ankit/features/multi\")\n",
        "\n",
        "if( not os.path.exists(SUBMISSION_FOLDER)):\n",
        "    os.makedirs(SUBMISSION_FOLDER)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "yTAotilTxevv",
        "outputId": "15a41e7a-d30c-42d0-9bc8-21581daf1a1e"
      },
      "source": [
        "test_multi_f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_multi_test_split_features.csv\"), index_col=0)\n",
        "test_multi_f1['token'] = test_multi_f1['token'].astype(str)\n",
        "test_multi_f1['sentence'] = test_multi_f1['sentence'].astype(str)\n",
        "test_multi_f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "test_multi_f1.drop(['parse', 'token1', 'token2', 'lemma1', 'lemma2', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
        "\n",
        "test_multi_f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_multi_test_preprocessed.csv\"), index_col=0)\n",
        "test_multi_f2['token'] = test_multi_f2['token'].astype(str)\n",
        "test_multi_f2['sentence'] = test_multi_f2['sentence'].astype(str)\n",
        "\n",
        "test_multi_features = test_multi_f1.merge(test_multi_f2, on=['id','sentence', 'corpus', 'token'])\n",
        "test_multi_features['token'] = test_multi_f2['token'].astype(str)\n",
        "test_multi_features.head(2)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL</th>\n",
              "      <td>come intending bring bound chief priest</td>\n",
              "      <td>bible</td>\n",
              "      <td>chief priest</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>JJ</td>\n",
              "      <td>0</td>\n",
              "      <td>NN</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39.121551</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>483.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302U8RURJZ1WF35NXY44RD66WL4NVH</th>\n",
              "      <td>day lord take away beauty anklet headband cres...</td>\n",
              "      <td>bible</td>\n",
              "      <td>crescent necklace</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>NN</td>\n",
              "      <td>1</td>\n",
              "      <td>NN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.830131</td>\n",
              "      <td>4.021996</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>268.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL            come intending bring bound chief priest  ...      483.0\n",
              "302U8RURJZ1WF35NXY44RD66WL4NVH  day lord take away beauty anklet headband cres...  ...      268.0\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi5GeQE0xevx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "d46b1621-d411-4435-e6e8-3cda9cdb1b23"
      },
      "source": [
        "# fill pos nan by NN, as they are in majority\n",
        "test_multi_features['pos2'] = test_multi_features['pos2'].fillna('NN')\n",
        "\n",
        "test_multi_features['pos1'] = test_multi_features['pos1'].apply((lambda x: labels[x]))\n",
        "test_multi_features['pos2'] = test_multi_features['pos2'].apply((lambda x: labels[x]))\n",
        "\n",
        "test_multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "    scaler.transform(test_multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "\n",
        "test_multi_features.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL</th>\n",
              "      <td>come intending bring bound chief priest</td>\n",
              "      <td>bible</td>\n",
              "      <td>chief priest</td>\n",
              "      <td>0.812903</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>0.281227</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.200765</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.217289</td>\n",
              "      <td>-0.582030</td>\n",
              "      <td>3.160928</td>\n",
              "      <td>0.209791</td>\n",
              "      <td>-0.519379</td>\n",
              "      <td>-0.342350</td>\n",
              "      <td>0.393219</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.957911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302U8RURJZ1WF35NXY44RD66WL4NVH</th>\n",
              "      <td>day lord take away beauty anklet headband cres...</td>\n",
              "      <td>bible</td>\n",
              "      <td>crescent necklace</td>\n",
              "      <td>1.962864</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>0.098326</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.553188</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>-0.493225</td>\n",
              "      <td>-0.553577</td>\n",
              "      <td>0.934757</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.121629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3UDTAB6HH6ZVX00DTRXAOJLWX0B094</th>\n",
              "      <td>unclean shall take ash burning sin offering ru...</td>\n",
              "      <td>bible</td>\n",
              "      <td>sin offering</td>\n",
              "      <td>0.812903</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>1.709990</td>\n",
              "      <td>2.397043</td>\n",
              "      <td>-0.098772</td>\n",
              "      <td>0.504082</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>-0.278444</td>\n",
              "      <td>1.837089</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.369612</td>\n",
              "      <td>-0.238985</td>\n",
              "      <td>0.113318</td>\n",
              "      <td>0.393219</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.053560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3L2OEKSTW9ASGQDOW725GFK5P77Y8D</th>\n",
              "      <td>precious treasure oil dwelling wise foolish ma...</td>\n",
              "      <td>bible</td>\n",
              "      <td>precious treasure</td>\n",
              "      <td>1.962864</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>0.281227</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>-0.098772</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-2.087483</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>0.109147</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.158920</td>\n",
              "      <td>-0.393608</td>\n",
              "      <td>-0.489569</td>\n",
              "      <td>2.559373</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.076898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39N6W9XWRDN795J6F5ET8S13DQKYGT</th>\n",
              "      <td>long god shall adversary reproach</td>\n",
              "      <td>bible</td>\n",
              "      <td>adversary reproach</td>\n",
              "      <td>2.192856</td>\n",
              "      <td>1.893924</td>\n",
              "      <td>-0.325016</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.249073</td>\n",
              "      <td>2.098228</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>-0.113199</td>\n",
              "      <td>-0.409235</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.264266</td>\n",
              "      <td>-0.489295</td>\n",
              "      <td>-0.547988</td>\n",
              "      <td>1.476296</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.920806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL            come intending bring bound chief priest  ...   0.957911\n",
              "302U8RURJZ1WF35NXY44RD66WL4NVH  day lord take away beauty anklet headband cres...  ...   0.121629\n",
              "3UDTAB6HH6ZVX00DTRXAOJLWX0B094  unclean shall take ash burning sin offering ru...  ...   0.053560\n",
              "3L2OEKSTW9ASGQDOW725GFK5P77Y8D  precious treasure oil dwelling wise foolish ma...  ...   0.076898\n",
              "39N6W9XWRDN795J6F5ET8S13DQKYGT                  long god shall adversary reproach  ...  -0.920806\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j18I8X1fxevy",
        "outputId": "86cdd2a1-9da7-488d-f9e1-c796df18754a"
      },
      "source": [
        "sentences_test_list = list(test_multi_features['sentence'])\n",
        "test_tokens_list = list(test_multi_features['token'])\n",
        "\n",
        "test_vectors = get_embeddings(sentences_test_list, test_tokens_list)\n",
        "print(test_vectors.shape)\n",
        "\n",
        "test_f_vectors = test_multi_features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "test_vectors = np.concatenate((test_vectors, test_f_vectors), axis=1)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(184, 300)\n",
            "(184, 320)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTMHt6NNxev2"
      },
      "source": [
        "# Linear Regression\n",
        "reg = LinearRegression().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/linear_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# Gradient Boosting\n",
        "reg = GradientBoostingRegressor(n_estimators=100).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/gradient_boosting_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# AdaBoost\n",
        "reg = AdaBoostRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/ada_boost_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# SVM regressor\n",
        "reg = SVR().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/SVM_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# MLP Regressor\n",
        "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-K2hsDwxev3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8738d7c6-0001-4b71-b485-4f7a697fcdfd"
      },
      "source": [
        "evaluate(SUBMISSION_FOLDER, FOLDER_PATH+\"/references/lcp_multi_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file linear_regression_baseline.csv\n",
            "pearson  :  0.80162468109306\n",
            "spearman :  0.7958259247466357\n",
            "mae      :  0.07648070707269523\n",
            "mse      :  0.009377504110086981\n",
            "r2       :  0.6114905425566605\n",
            "\n",
            "For file gradient_boosting_baseline.csv\n",
            "pearson  :  0.7425749839115788\n",
            "spearman :  0.7295822333985482\n",
            "mae      :  0.08519289095213395\n",
            "mse      :  0.011636223379959393\n",
            "r2       :  0.517911932752478\n",
            "\n",
            "For file ada_boost_baseline.csv\n",
            "pearson  :  0.7205928299840861\n",
            "spearman :  0.722019865932745\n",
            "mae      :  0.09487093040550654\n",
            "mse      :  0.013532488729031253\n",
            "r2       :  0.43934976809887283\n",
            "\n",
            "For file MLP_baseline.csv\n",
            "pearson  :  0.5627625010663627\n",
            "spearman :  0.5379100633691584\n",
            "mae      :  0.11296352326760921\n",
            "mse      :  0.02100409896531524\n",
            "r2       :  0.1298013845365148\n",
            "\n",
            "For file SVM_baseline.csv\n",
            "pearson  :  0.81236069308495\n",
            "spearman :  0.7997878887495248\n",
            "mae      :  0.07301231575210401\n",
            "mse      :  0.008358819519049489\n",
            "r2       :  0.6536945867376867\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}