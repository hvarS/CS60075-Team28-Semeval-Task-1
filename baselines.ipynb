{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from eval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/preprocessed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1 - Single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3ZLW647WALVGE8EBR50EGUBPU4P32A</th>\n",
       "      <td>bible</td>\n",
       "      <td>behold came river seven cattle sleek fat fed m...</td>\n",
       "      <td>river</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34R0BODSP1ZBN3DVY8J8XSIY551E5C</th>\n",
       "      <td>bible</td>\n",
       "      <td>fellow bondservant brother prophet keep word book</td>\n",
       "      <td>brother</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</th>\n",
       "      <td>bible</td>\n",
       "      <td>man lord land said u know honest men leave one...</td>\n",
       "      <td>brother</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3BFNCI9LYKQN09BHXHH9CLSX5KP738</th>\n",
       "      <td>bible</td>\n",
       "      <td>shimei sixteen son six daughter brother didnt ...</td>\n",
       "      <td>brother</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</th>\n",
       "      <td>bible</td>\n",
       "      <td>put brother far</td>\n",
       "      <td>brother</td>\n",
       "      <td>0.263889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               corpus  \\\n",
       "id                                      \n",
       "3ZLW647WALVGE8EBR50EGUBPU4P32A  bible   \n",
       "34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible   \n",
       "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible   \n",
       "3BFNCI9LYKQN09BHXHH9CLSX5KP738  bible   \n",
       "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  bible   \n",
       "\n",
       "                                                                         sentence  \\\n",
       "id                                                                                  \n",
       "3ZLW647WALVGE8EBR50EGUBPU4P32A  behold came river seven cattle sleek fat fed m...   \n",
       "34R0BODSP1ZBN3DVY8J8XSIY551E5C  fellow bondservant brother prophet keep word book   \n",
       "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  man lord land said u know honest men leave one...   \n",
       "3BFNCI9LYKQN09BHXHH9CLSX5KP738  shimei sixteen son six daughter brother didnt ...   \n",
       "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2                                    put brother far   \n",
       "\n",
       "                                  token  complexity  \n",
       "id                                                   \n",
       "3ZLW647WALVGE8EBR50EGUBPU4P32A    river    0.000000  \n",
       "34R0BODSP1ZBN3DVY8J8XSIY551E5C  brother    0.000000  \n",
       "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  brother    0.050000  \n",
       "3BFNCI9LYKQN09BHXHH9CLSX5KP738  brother    0.150000  \n",
       "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  brother    0.263889  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_folder, \"lcp_single_train_preprocessed.csv\"), index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens : 2856\n",
      "130 tokens not in vocab\n"
     ]
    }
   ],
   "source": [
    "# take mean of duplicate tokens\n",
    "# token not in vocab are considered as 'unk'\n",
    "\n",
    "df = df.groupby('token').mean().reset_index()\n",
    "print(\"unique tokens : {}\".format(len(df)))\n",
    "\n",
    "count = [True if w in word_vector else False for w in df['token']]\n",
    "print(\"{} tokens not in vocab\".format(len(df)-sum(count)))\n",
    "\n",
    "vectors = np.array(list(df['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 tokens not in vocab\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(data_folder, \"lcp_single_test_preprocessed.csv\"), index_col=0)\n",
    "count = [True if w in word_vector else False for w in test_df['token']]\n",
    "print(\"{} tokens not in vocab\".format(len(test_df)-sum(count)))\n",
    "\n",
    "testdf_vectors = np.array(list(test_df['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_folder = \"predictions/single\"\n",
    "\n",
    "# Linear Regression\n",
    "reg = LinearRegression().fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/linear_regression_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "reg = GradientBoostingRegressor().fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/gradient_boosting_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM regressor\n",
    "reg = SVR().fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/SVM_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Regressor\n",
    "regr = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/MLP_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file gradient_boosting_baseline.csv\n",
      "pearson  :  0.6475249120262303\n",
      "spearman :  0.625628747127938\n",
      "mae      :  0.0746743972587837\n",
      "mse      :  0.009492645287416529\n",
      "r2       :  0.4134939111449625\n",
      "\n",
      "For file SVM_baseline.csv\n",
      "pearson  :  0.6553534622949289\n",
      "spearman :  0.609787415042385\n",
      "mae      :  0.07787217407123892\n",
      "mse      :  0.009971588727391003\n",
      "r2       :  0.3839022393551641\n",
      "\n",
      "For file linear_regression_baseline.csv\n",
      "pearson  :  0.6639716433790339\n",
      "spearman :  0.6391991602453316\n",
      "mae      :  0.07316730116994907\n",
      "mse      :  0.009196928531600866\n",
      "r2       :  0.4317648643525217\n",
      "\n",
      "For file MLP_baseline.csv\n",
      "pearson  :  0.6553534622949289\n",
      "spearman :  0.609787415042385\n",
      "mae      :  0.07787217407123892\n",
      "mse      :  0.009971588727391003\n",
      "r2       :  0.3839022393551641\n"
     ]
    }
   ],
   "source": [
    "evaluate(submission_folder, \"references/lcp_single_test_labelled_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2 - Multi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</th>\n",
       "      <td>bible</td>\n",
       "      <td>seventh day sabbath yahweh god shall work son ...</td>\n",
       "      <td>seventh day</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3WGCNLZJKF877FYC1Q6COKNWTDWD11</th>\n",
       "      <td>bible</td>\n",
       "      <td>let man test own work take pride neighbor</td>\n",
       "      <td>own work</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3UOMW19E6D6WQ5TH2HDD74IVKTP5CB</th>\n",
       "      <td>bible</td>\n",
       "      <td>understanding made heaven loving kindness endu...</td>\n",
       "      <td>loving kindness</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36JW4WBR06KF9AXMUL4N476OMF8FHD</th>\n",
       "      <td>bible</td>\n",
       "      <td>remember god also spare according greatness lo...</td>\n",
       "      <td>loving kindness</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A</th>\n",
       "      <td>bible</td>\n",
       "      <td>loving kindness better life lip shall praise</td>\n",
       "      <td>loving kindness</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               corpus  \\\n",
       "id                                      \n",
       "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  bible   \n",
       "3WGCNLZJKF877FYC1Q6COKNWTDWD11  bible   \n",
       "3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  bible   \n",
       "36JW4WBR06KF9AXMUL4N476OMF8FHD  bible   \n",
       "3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A  bible   \n",
       "\n",
       "                                                                         sentence  \\\n",
       "id                                                                                  \n",
       "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  seventh day sabbath yahweh god shall work son ...   \n",
       "3WGCNLZJKF877FYC1Q6COKNWTDWD11          let man test own work take pride neighbor   \n",
       "3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  understanding made heaven loving kindness endu...   \n",
       "36JW4WBR06KF9AXMUL4N476OMF8FHD  remember god also spare according greatness lo...   \n",
       "3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A       loving kindness better life lip shall praise   \n",
       "\n",
       "                                          token  complexity  \n",
       "id                                                           \n",
       "3S37Y8CWI80N8KVM53U4E6JKCDC4WE      seventh day    0.027778  \n",
       "3WGCNLZJKF877FYC1Q6COKNWTDWD11         own work    0.050000  \n",
       "3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  loving kindness    0.050000  \n",
       "36JW4WBR06KF9AXMUL4N476OMF8FHD  loving kindness    0.050000  \n",
       "3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A  loving kindness    0.075000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_folder, \"lcp_multi_train_preprocessed.csv\"), index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens : 1244\n",
      "33 tokens not in vocab\n"
     ]
    }
   ],
   "source": [
    "# take avg of word embeddings in a token\n",
    "\n",
    "df = df.groupby('token').mean().reset_index()\n",
    "print(\"unique tokens : {}\".format(len(df)))\n",
    "\n",
    "count0 = [True if w.split()[0] in word_vector else False for w in df['token']]\n",
    "count1 = [True if w.split()[1] in word_vector else False for w in df['token']]\n",
    "print(\"{} tokens not in vocab\".format(2*len(df)-sum(count0)-sum(count1)))\n",
    "\n",
    "vectors = np.zeros((len(df),300))\n",
    "\n",
    "for i,token in enumerate(df['token']):\n",
    "    for x in token.split():\n",
    "        vectors[i] += word_vector[x] if x in word_vector else word_vector['unk']\n",
    "    vectors[i] /= len(token.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tokens not in vocab\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(data_folder, \"lcp_multi_test_preprocessed.csv\"), index_col=0)\n",
    "\n",
    "count0 = [True if w.split()[0] in word_vector else False for w in test_df['token']]\n",
    "count1 = [True if w.split()[1] in word_vector else False for w in test_df['token']]\n",
    "print(\"{} tokens not in vocab\".format(2*len(test_df)-sum(count0)-sum(count1)))\n",
    "\n",
    "testdf_vectors = np.zeros((len(test_df),300))\n",
    "for i,token in enumerate(test_df['token']):\n",
    "    for x in token.split():\n",
    "        testdf_vectors[i] += word_vector[x] if x in word_vector else word_vector['unk']\n",
    "    testdf_vectors[i] /= len(token.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_folder = \"predictions/multi\"\n",
    "\n",
    "# Linear Regression\n",
    "reg = LinearRegression().fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/linear_regression_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "reg = GradientBoostingRegressor().fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/gradient_boosting_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM regressor\n",
    "reg = SVR().fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/SVM_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Regressor\n",
    "regr = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(df['complexity']))\n",
    "y_pred = reg.predict(testdf_vectors)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":test_df.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/MLP_baseline.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file gradient_boosting_baseline.csv\n",
      "pearson  :  0.7266561437943603\n",
      "spearman :  0.6739876494256528\n",
      "mae      :  0.08769937390012252\n",
      "mse      :  0.012007532002936954\n",
      "r2       :  0.50252863779856\n",
      "\n",
      "For file SVM_baseline.csv\n",
      "pearson  :  0.7308921844093375\n",
      "spearman :  0.7011304191917298\n",
      "mae      :  0.08428310274484914\n",
      "mse      :  0.011455105899844999\n",
      "r2       :  0.5254156195658017\n",
      "\n",
      "For file linear_regression_baseline.csv\n",
      "pearson  :  0.6605462208340676\n",
      "spearman :  0.6464161980672312\n",
      "mae      :  0.09322292165368064\n",
      "mse      :  0.013838118656466326\n",
      "r2       :  0.42668753773434287\n",
      "\n",
      "For file MLP_baseline.csv\n",
      "pearson  :  0.7308921844093375\n",
      "spearman :  0.7011304191917298\n",
      "mae      :  0.08428310274484914\n",
      "mse      :  0.011455105899844999\n",
      "r2       :  0.5254156195658017\n"
     ]
    }
   ],
   "source": [
    "evaluate(submission_folder, \"references/lcp_multi_test_labelled_preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpTask1",
   "language": "python",
   "name": "nlptask1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
