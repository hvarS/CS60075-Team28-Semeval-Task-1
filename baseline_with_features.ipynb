{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_with_features.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ACPhKHqRyuby",
        "be6X9hcEzOSn",
        "_1wXzYgmV2Mx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACPhKHqRyuby"
      },
      "source": [
        "## Initial Setup - Imports and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0RojGMRyMi-",
        "outputId": "21cfd0c8-d756-44f2-93ea-64ba95446ea4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efMuzwW5yjN9",
        "outputId": "483c90ab-0c41-4e5f-bc11-dd515f3edce9"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d embeddings"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 06:49:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-12 06:49:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-12 06:49:04--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.68MB/s    in 2m 41s  \n",
            "\n",
            "2021-04-12 06:51:45 (5.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: embeddings/glove.6B.50d.txt  \n",
            "  inflating: embeddings/glove.6B.100d.txt  \n",
            "  inflating: embeddings/glove.6B.200d.txt  \n",
            "  inflating: embeddings/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBZsjocky1Gi"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMS4PuEy1ev"
      },
      "source": [
        "FOLDER_PATH = \"/content/drive/MyDrive/CS60075-Team28-Task-1\"\n",
        "DATA_FOLDER = os.path.join(FOLDER_PATH,\"data/preprocessed\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSvgOs0sy6Dh"
      },
      "source": [
        "# import evaluate function\n",
        "import sys\n",
        "sys.path.append(FOLDER_PATH)\n",
        "from eval import evaluate"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6X9hcEzOSn"
      },
      "source": [
        "## Functions to Read GloVe Embeddings and Extract them According to sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dWQIdr5zEeP",
        "outputId": "609a3be9-6d7c-4e81-ca1c-431f5f02722b"
      },
      "source": [
        "def read_glove_vector(glove_vec):\n",
        "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
        "    words = set()\n",
        "    word_to_vec_map = {}\n",
        "    for line in f:\n",
        "      w_line = line.split()\n",
        "      curr_word = w_line[0]\n",
        "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
        "\n",
        "  return word_to_vec_map\n",
        "\n",
        "word_to_vec_map = read_glove_vector('embeddings/glove.6B.300d.txt')\n",
        "print(len(word_to_vec_map),\" words loaded!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000  words loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4_p1cvuzaj3"
      },
      "source": [
        "def get_embeddings(sentences, tokens):\n",
        "    token_emb = []\n",
        "    for s,t in zip(sentences, tokens):\n",
        "        \n",
        "        # fill unk by nan\n",
        "        # calculate mean over non nan embeddings\n",
        "        # fill unk by the mean embedding of sentence\n",
        "        # pad 0 vectors till max_len\n",
        "        \n",
        "        temp_emb = [ word_to_vec_map[x] if x in word_to_vec_map else np.full((300,), np.nan) for x in t.split() ]\n",
        "        \n",
        "        # calculate mean for filling null values <unk>\n",
        "        temp_sent_emb = [ word_to_vec_map[x] if x in word_to_vec_map else np.full((300,), np.nan) for x in s.split() ]\n",
        "        mean_emb = np.nanmean(np.array(temp_sent_emb), axis=0)\n",
        "        \n",
        "        # single or multi - will be converted to (1,300) \n",
        "        temp_emb = np.mean(np.array([ mean_emb if np.isnan(x[0]) else x for x in temp_emb ]), axis=0)\n",
        "\n",
        "        token_emb.append(temp_emb)\n",
        "\n",
        "    return np.array(token_emb)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1wXzYgmV2Mx"
      },
      "source": [
        "## Load data with Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmAPF6kVbdX8",
        "outputId": "983e10c8-40b7-4604-c05c-7bce3fc740a3"
      },
      "source": [
        "f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_single_train_features.csv\"), index_col=0)\n",
        "f1['token'] = f1['token'].astype(str)\n",
        "f1['sentence'] = f1['sentence'].astype(str)\n",
        "f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "f1.drop(['parse', 'lemma'], axis=1, inplace=True)\n",
        "\n",
        "print(f1.columns)\n",
        "\n",
        "f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_single_train_preprocessed.csv\"), index_col=0)\n",
        "f2['token'] = f2['token'].astype(str)\n",
        "f2['sentence'] = f2['sentence'].astype(str)\n",
        "print(f2.columns)\n",
        "\n",
        "features = f1.merge(f2, on=['id','sentence', 'corpus', 'token', 'complexity'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['sentence', 'corpus', 'token', 'complexity', 'token_length',\n",
            "       'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms',\n",
            "       'google frequency'],\n",
            "      dtype='object')\n",
            "Index(['corpus', 'sentence', 'token', 'complexity', 'biomedical', 'bible',\n",
            "       'subtitles', 'wiki', 'familarity'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "-G3zFmbnhPqB",
        "outputId": "e5cc231f-d845-4a12-9269-5ebf38070c92"
      },
      "source": [
        "# fill pos nan by NN, as they are in majority\n",
        "features['pos'] = features['pos'].fillna('NN')\n",
        "features['token_length'] = features['token_length'].fillna(0)\n",
        "\n",
        "# categorical encoding\n",
        "labels = dict(features['pos'].value_counts())\n",
        "labels = { k:i for i,k in enumerate(labels)}\n",
        "labels['POS'] = len(labels)\n",
        "print(labels)\n",
        "\n",
        "\n",
        "def get_vowels(word):\n",
        "    val = 0\n",
        "    for w in word:\n",
        "        if(w in ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U','u']):\n",
        "            val+=1\n",
        "    return val\n",
        "\n",
        "features['token_vowels'] = features['token'].apply(lambda x: get_vowels(x) )\n",
        "\n",
        "\n",
        "features['pos'] = features['pos'].apply((lambda x: labels[x]))\n",
        "\n",
        "# scaler = preprocessing.StandardScaler()\n",
        "# features[['token_length', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'familarity', 'token_vowels']] =  \\\n",
        "#     scaler.fit_transform(features[['token_length', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'familarity', 'token_vowels']])\n",
        "\n",
        "\n",
        "features.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'NN': 0, 'JJ': 1, 'NNP': 2, 'NNS': 3, 'VBG': 4, 'VB': 5, 'RB': 6, 'VBP': 7, 'UH': 8, 'CD': 9, 'VBN': 10, 'FW': 11, 'VBZ': 12, 'IN': 13, 'JJR': 14, 'VBD': 15, 'PRP': 16, 'GW': 17, 'MD': 18, 'LS': 19, 'WRB': 20, 'JJS': 21, 'AFX': 22, 'POS': 23}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos</th>\n",
              "      <th>dep num</th>\n",
              "      <th>synonyms</th>\n",
              "      <th>hypernyms</th>\n",
              "      <th>hyponyms</th>\n",
              "      <th>google frequency</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "      <th>token_vowels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3ZLW647WALVGE8EBR50EGUBPU4P32A</th>\n",
              "      <td>behold came river seven cattle sleek fat fed m...</td>\n",
              "      <td>bible</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>173.485953</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>565</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34R0BODSP1ZBN3DVY8J8XSIY551E5C</th>\n",
              "      <td>fellow bondservant brother prophet keep word book</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</th>\n",
              "      <td>man lord land said u know honest men leave one...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3BFNCI9LYKQN09BHXHH9CLSX5KP738</th>\n",
              "      <td>shimei sixteen son six daughter brother didnt ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</th>\n",
              "      <td>put brother far</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>112.198857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... token_vowels\n",
              "id                                                                                 ...             \n",
              "3ZLW647WALVGE8EBR50EGUBPU4P32A  behold came river seven cattle sleek fat fed m...  ...            2\n",
              "34R0BODSP1ZBN3DVY8J8XSIY551E5C  fellow bondservant brother prophet keep word book  ...            2\n",
              "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  man lord land said u know honest men leave one...  ...            2\n",
              "3BFNCI9LYKQN09BHXHH9CLSX5KP738  shimei sixteen son six daughter brother didnt ...  ...            2\n",
              "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2                                    put brother far  ...            2\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "ocBaxNJ1uPD-",
        "outputId": "55f473a7-d296-4448-d904-a543810c1e88"
      },
      "source": [
        "multi_f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_multi_train_split_features.csv\"), index_col=0)\n",
        "multi_f1['token'] = multi_f1['token'].astype(str)\n",
        "multi_f1['sentence'] = multi_f1['sentence'].astype(str)\n",
        "multi_f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "multi_f1.drop(['parse', 'token1', 'token2', 'lemma1', 'lemma2', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
        "\n",
        "multi_f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_multi_train_preprocessed.csv\"), index_col=0)\n",
        "multi_f2['token'] = multi_f2['token'].astype(str)\n",
        "multi_f2['sentence'] = multi_f2['sentence'].astype(str)\n",
        "\n",
        "multi_features = multi_f1.merge(multi_f2, on=['id','sentence', 'corpus', 'token', 'complexity'])\n",
        "multi_features.head(2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</th>\n",
              "      <td>seventh day sabbath yahweh god shall work son ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>seventh day</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>JJ</td>\n",
              "      <td>0</td>\n",
              "      <td>NN</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>24.522564</td>\n",
              "      <td>682.298213</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>297.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3WGCNLZJKF877FYC1Q6COKNWTDWD11</th>\n",
              "      <td>let man test own work take pride neighbor</td>\n",
              "      <td>bible</td>\n",
              "      <td>own work</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>JJ</td>\n",
              "      <td>0</td>\n",
              "      <td>NN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1022.711588</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>600.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  seventh day sabbath yahweh god shall work son ...  ...      297.5\n",
              "3WGCNLZJKF877FYC1Q6COKNWTDWD11          let man test own work take pride neighbor  ...      600.5\n",
              "\n",
              "[2 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6oRx6fW5uxx1",
        "outputId": "bcb2627c-44d9-4987-c9dd-e666be278d29"
      },
      "source": [
        "# fill pos nan by NN, as they are in majority\n",
        "multi_features['pos2'] = multi_features['pos2'].fillna('NN')\n",
        "\n",
        "multi_features['pos1'] = multi_features['pos1'].apply((lambda x: labels[x]))\n",
        "multi_features['pos2'] = multi_features['pos2'].apply((lambda x: labels[x]))\n",
        "\n",
        "# scaler = preprocessing.StandardScaler()\n",
        "# multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "#     scaler.fit_transform(multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "\n",
        "multi_features.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</th>\n",
              "      <td>seventh day sabbath yahweh god shall work son ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>seventh day</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>24.522564</td>\n",
              "      <td>682.298213</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>297.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3WGCNLZJKF877FYC1Q6COKNWTDWD11</th>\n",
              "      <td>let man test own work take pride neighbor</td>\n",
              "      <td>bible</td>\n",
              "      <td>own work</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1022.711588</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>600.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3UOMW19E6D6WQ5TH2HDD74IVKTP5CB</th>\n",
              "      <td>understanding made heaven loving kindness endu...</td>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>339.132903</td>\n",
              "      <td>14.221491</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>566.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36JW4WBR06KF9AXMUL4N476OMF8FHD</th>\n",
              "      <td>remember god also spare according greatness lo...</td>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>339.132903</td>\n",
              "      <td>14.221491</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>566.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A</th>\n",
              "      <td>loving kindness better life lip shall praise</td>\n",
              "      <td>bible</td>\n",
              "      <td>loving kindness</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>339.132903</td>\n",
              "      <td>14.221491</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>566.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3S37Y8CWI80N8KVM53U4E6JKCDC4WE  seventh day sabbath yahweh god shall work son ...  ...      297.5\n",
              "3WGCNLZJKF877FYC1Q6COKNWTDWD11          let man test own work take pride neighbor  ...      600.5\n",
              "3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  understanding made heaven loving kindness endu...  ...      566.5\n",
              "36JW4WBR06KF9AXMUL4N476OMF8FHD  remember god also spare according greatness lo...  ...      566.5\n",
              "3HRWUH63QU2FH9Q8R7MRNFC7JX2N5A       loving kindness better life lip shall praise  ...      566.5\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "kCfgHQOv3-r_",
        "outputId": "672e3bcb-5b4b-4664-8799-5224d0ba3dc1"
      },
      "source": [
        "features['pos1'] = features['pos'].copy()\n",
        "features['pos2'] = features['pos'] \n",
        "\n",
        "features['dep num1'] = features['dep num'] \n",
        "features['dep num2'] = features['dep num'] \n",
        "\n",
        "features['synonyms1'] = features['synonyms'] \n",
        "features['synonyms2'] = features['synonyms'] \n",
        "\n",
        "features['hypernyms1'] = features['hypernyms'] \n",
        "features['hypernyms2'] = features['hypernyms'] \n",
        "\n",
        "features['hyponyms1'] = features['hyponyms'] \n",
        "features['hyponyms2'] = features['hyponyms'] \n",
        "\n",
        "features['google frequency1'] = features['google frequency'] \n",
        "features['google frequency2'] = features['google frequency1'] \n",
        "\n",
        "features.drop(['pos','dep num', 'synonyms', 'hyponyms', 'hypernyms', 'google frequency'], axis=1, inplace=True)\n",
        "\n",
        "features = features.append( multi_features)\n",
        "print(len(features))\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "    scaler.fit_transform(features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "features.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>pos1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3ZLW647WALVGE8EBR50EGUBPU4P32A</th>\n",
              "      <td>behold came river seven cattle sleek fat fed m...</td>\n",
              "      <td>bible</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.797042</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.276865</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.831106</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>-0.369612</td>\n",
              "      <td>0.419993</td>\n",
              "      <td>0.466245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34R0BODSP1ZBN3DVY8J8XSIY551E5C</th>\n",
              "      <td>fellow bondservant brother prophet keep word book</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>1.709990</td>\n",
              "      <td>1.365895</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</th>\n",
              "      <td>man lord land said u know honest men leave one...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>0.098326</td>\n",
              "      <td>-0.098772</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3BFNCI9LYKQN09BHXHH9CLSX5KP738</th>\n",
              "      <td>shimei sixteen son six daughter brother didnt ...</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>2.515822</td>\n",
              "      <td>2.098228</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</th>\n",
              "      <td>put brother far</td>\n",
              "      <td>bible</td>\n",
              "      <td>brother</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>-0.337058</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.405225</td>\n",
              "      <td>-0.689858</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>0.904158</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-0.063647</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.168281</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>0.088143</td>\n",
              "      <td>0.097424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... google frequency2\n",
              "id                                                                                 ...                  \n",
              "3ZLW647WALVGE8EBR50EGUBPU4P32A  behold came river seven cattle sleek fat fed m...  ...          0.466245\n",
              "34R0BODSP1ZBN3DVY8J8XSIY551E5C  fellow bondservant brother prophet keep word book  ...          0.097424\n",
              "3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  man lord land said u know honest men leave one...  ...          0.097424\n",
              "3BFNCI9LYKQN09BHXHH9CLSX5KP738  shimei sixteen son six daughter brother didnt ...  ...          0.097424\n",
              "3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2                                    put brother far  ...          0.097424\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7b6opCo3nEs"
      },
      "source": [
        "## Testing Single Word Complexity Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLMDTsyj3np9",
        "outputId": "19a17bce-b019-4d47-d604-9bdb7d53b30c"
      },
      "source": [
        "sentences_train_list = list(features['sentence'])\n",
        "complexity_train_list = list(features['complexity'])\n",
        "tokens_train_list = list(features['token'])\n",
        "\n",
        "vectors = get_embeddings(sentences_train_list, tokens_train_list)\n",
        "print(vectors.shape)\n",
        "\n",
        "# f_vectors = features[['token_length', 'token_vowels', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "# or \n",
        "\n",
        "f_vectors = features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "\n",
        "print(f_vectors.shape)\n",
        "vectors = np.concatenate((vectors, f_vectors), axis=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9179, 300)\n",
            "(9179, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6OJ6MJ93tZe"
      },
      "source": [
        "SUBMISSION_FOLDER = os.path.join(FOLDER_PATH,\"predictions/baselines/features/single\")\n",
        "\n",
        "if( not os.path.exists(SUBMISSION_FOLDER)):\n",
        "    os.makedirs(SUBMISSION_FOLDER)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfDuoMtQ4ccf"
      },
      "source": [
        "test_f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_single_test_features.csv\"), index_col=0)\n",
        "test_f1['token'] = test_f1['token'].astype(str)\n",
        "test_f1['sentence'] = test_f1['sentence'].astype(str)\n",
        "test_f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "test_f1.drop(['parse', 'lemma'], axis=1, inplace=True)\n",
        "\n",
        "test_f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_single_test_preprocessed.csv\"), index_col=0)\n",
        "test_f2['token'] = test_f2['token'].astype(str)\n",
        "test_f2['sentence'] = test_f2['sentence'].astype(str)\n",
        "\n",
        "test_features = test_f1.merge(test_f2, on=['id','sentence', 'corpus', 'token'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZaDUyQ4fkW"
      },
      "source": [
        "# fill pos nan by NN, as they are in majority\n",
        "test_features['pos'] = test_features['pos'].fillna('NN')\n",
        "test_features['token_length'] = test_features['token_length'].fillna(0)\n",
        "\n",
        "test_features['pos'] = test_features['pos'].apply((lambda x: labels[x]))\n",
        "\n",
        "def get_vowels(word):\n",
        "    val = 0\n",
        "    for w in word:\n",
        "        if(w in ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U','u']):\n",
        "            val+=1\n",
        "    return val\n",
        "\n",
        "test_features['token_vowels'] = test_features['token'].apply(lambda x: get_vowels(x) )\n",
        "\n",
        "test_features['pos1'] = test_features['pos'].copy()\n",
        "test_features['pos2'] = test_features['pos'] \n",
        "\n",
        "test_features['dep num1'] = test_features['dep num'] \n",
        "test_features['dep num2'] = test_features['dep num'] \n",
        "\n",
        "test_features['synonyms1'] = test_features['synonyms'] \n",
        "test_features['synonyms2'] = test_features['synonyms'] \n",
        "\n",
        "test_features['hypernyms1'] = test_features['hypernyms'] \n",
        "test_features['hypernyms2'] = test_features['hypernyms'] \n",
        "\n",
        "test_features['hyponyms1'] = test_features['hyponyms'] \n",
        "test_features['hyponyms2'] = test_features['hyponyms'] \n",
        "\n",
        "test_features['google frequency1'] = test_features['google frequency'] \n",
        "test_features['google frequency2'] = test_features['google frequency'] \n",
        "\n",
        "test_features.drop(['pos','dep num', 'synonyms', 'hyponyms', 'hypernyms', 'google frequency'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "test_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "    scaler.transform(test_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44LS-PRw4nx3",
        "outputId": "6eaa8c67-ba71-48e8-eb52-d619111245cc"
      },
      "source": [
        "sentences_test_list = list(test_features['sentence'])\n",
        "test_tokens_list = list(test_features['token'])\n",
        "\n",
        "test_vectors = get_embeddings(sentences_test_list, test_tokens_list)\n",
        "print(test_vectors.shape)\n",
        "\n",
        "# test_f_vectors = test_features[['token_length', 'token_vowels', 'syllables', 'pos', 'dep num', 'synonyms', 'hypernyms', 'hyponyms', 'google frequency', 'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "\n",
        "test_f_vectors = test_features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "\n",
        "test_vectors = np.concatenate((test_vectors, test_f_vectors), axis=1)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(917, 300)\n",
            "(917, 320)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDnOZZ484zlD"
      },
      "source": [
        "# # # Gradient Boosting\n",
        "reg = GradientBoostingRegressor(n_estimators=250).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/gradient_boosting_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# Linear Regression\n",
        "reg = LinearRegression().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/linear_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# # xgb Regression\n",
        "# from xgboost import XGBRegressor\n",
        "# reg = XGBRegressor(objective ='reg:squarederror', n_estimators=250).fit(vectors, np.array(complexity_train_list))\n",
        "# y_pred = reg.predict(test_vectors)\n",
        "\n",
        "# pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "# pred.to_csv(SUBMISSION_FOLDER+\"/xgb_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "\n",
        "# # AdaBoost\n",
        "reg = AdaBoostRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/ada_boost_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# SVM regressor\n",
        "reg = SVR().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/SVM_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# MLP Regressor\n",
        "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF8xgAeH44jY",
        "outputId": "a0037bd2-5c88-45be-811c-d79f2c8298d2"
      },
      "source": [
        "evaluate(SUBMISSION_FOLDER, FOLDER_PATH+\"/references/lcp_single_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file gradient_boosting_baseline.csv\n",
            "pearson  :  0.7327576203283477\n",
            "spearman :  0.7005231534638561\n",
            "mae      :  0.06711175279397771\n",
            "mse      :  0.007541193308956854\n",
            "r2       :  0.5340649883126738\n",
            "\n",
            "For file linear_regression_baseline.csv\n",
            "pearson  :  0.6873590114098053\n",
            "spearman :  0.6756264174128696\n",
            "mae      :  0.07243309012545256\n",
            "mse      :  0.008763922190523868\n",
            "r2       :  0.45851829797035115\n",
            "\n",
            "For file ada_boost_baseline.csv\n",
            "pearson  :  0.6939101001270307\n",
            "spearman :  0.6816027968019428\n",
            "mae      :  0.07241640271567897\n",
            "mse      :  0.008909605803147074\n",
            "r2       :  0.44951718992693\n",
            "\n",
            "For file SVM_baseline.csv\n",
            "pearson  :  0.7039790652215149\n",
            "spearman :  0.6825872094709771\n",
            "mae      :  0.0707195319367411\n",
            "mse      :  0.008285997725066644\n",
            "r2       :  0.48804701209759505\n",
            "\n",
            "For file MLP_baseline.csv\n",
            "pearson  :  0.4797932186467725\n",
            "spearman :  0.4510805101895456\n",
            "mae      :  0.11374913835819352\n",
            "mse      :  0.02234535372724599\n",
            "r2       :  -0.38061474139587914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "dn5PoYNq4_zQ",
        "outputId": "c0a08c0d-f92b-458b-9317-bcf0b0b5ee51"
      },
      "source": [
        "'''\n",
        "For file gradient_boosting_baseline.csv\n",
        "pearson  :  0.7327576203283477\n",
        "spearman :  0.7005231534638561\n",
        "mae      :  0.06711175279397771\n",
        "mse      :  0.007541193308956854\n",
        "r2       :  0.5340649883126738\n",
        "'''"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nFor file gradient_boosting_baseline.csv\\npearson  :  0.7327576203283477\\nspearman :  0.7005231534638561\\nmae      :  0.06711175279397771\\nmse      :  0.007541193308956854\\nr2       :  0.5340649883126738\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhICpOTs6pER"
      },
      "source": [
        "## Tesing Multi Word Complexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RJPpLuN6r8x",
        "outputId": "209e4abf-a625-4000-a726-a5f4c6f4634f"
      },
      "source": [
        "sentences_train_list = list(multi_features['sentence'])\n",
        "complexity_train_list = list(multi_features['complexity'])\n",
        "tokens_train_list = list(multi_features['token'])\n",
        "\n",
        "vectors = get_embeddings(sentences_train_list, tokens_train_list)\n",
        "print(vectors.shape)\n",
        "\n",
        "f_vectors = multi_features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "print(f_vectors.shape)\n",
        "vectors = np.concatenate((vectors, f_vectors), axis=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1517, 300)\n",
            "(1517, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtg3m2rS6uPs"
      },
      "source": [
        "SUBMISSION_FOLDER = os.path.join(FOLDER_PATH,\"predictions/baselines/features/multi\")\n",
        "\n",
        "if( not os.path.exists(SUBMISSION_FOLDER)):\n",
        "    os.makedirs(SUBMISSION_FOLDER)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "6OlRvj6E64Eh",
        "outputId": "cca03ce5-70fb-4d97-8f09-214446297c29"
      },
      "source": [
        "test_multi_f1 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/extra_features/lcp_multi_test_split_features.csv\"), index_col=0)\n",
        "test_multi_f1['token'] = test_multi_f1['token'].astype(str)\n",
        "test_multi_f1['sentence'] = test_multi_f1['sentence'].astype(str)\n",
        "test_multi_f1.set_index(\"id\", inplace=True)\n",
        "\n",
        "# drop unwanted features\n",
        "test_multi_f1.drop(['parse', 'token1', 'token2', 'lemma1', 'lemma2', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
        "\n",
        "test_multi_f2 = pd.read_csv(os.path.join(FOLDER_PATH, \"data/added_corpus_presence/lcp_multi_test_preprocessed.csv\"), index_col=0)\n",
        "test_multi_f2['token'] = test_multi_f2['token'].astype(str)\n",
        "test_multi_f2['sentence'] = test_multi_f2['sentence'].astype(str)\n",
        "\n",
        "test_multi_features = test_multi_f1.merge(test_multi_f2, on=['id','sentence', 'corpus', 'token'])\n",
        "test_multi_features['token'] = test_multi_f2['token'].astype(str)\n",
        "test_multi_features.head(2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL</th>\n",
              "      <td>come intending bring bound chief priest</td>\n",
              "      <td>bible</td>\n",
              "      <td>chief priest</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>JJ</td>\n",
              "      <td>0</td>\n",
              "      <td>NN</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39.121551</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>483.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302U8RURJZ1WF35NXY44RD66WL4NVH</th>\n",
              "      <td>day lord take away beauty anklet headband cres...</td>\n",
              "      <td>bible</td>\n",
              "      <td>crescent necklace</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>NN</td>\n",
              "      <td>1</td>\n",
              "      <td>NN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.830131</td>\n",
              "      <td>4.021996</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>268.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL            come intending bring bound chief priest  ...      483.0\n",
              "302U8RURJZ1WF35NXY44RD66WL4NVH  day lord take away beauty anklet headband cres...  ...      268.0\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "qVvBHJVf69bF",
        "outputId": "545b81fb-5ebd-462f-fd5f-90e84df98536"
      },
      "source": [
        "test_multi_features['pos2'] = test_multi_features['pos2'].fillna('NN')\n",
        "\n",
        "test_multi_features['pos1'] = test_multi_features['pos1'].apply((lambda x: labels[x]))\n",
        "test_multi_features['pos2'] = test_multi_features['pos2'].apply((lambda x: labels[x]))\n",
        "\n",
        "test_multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']] =  \\\n",
        "    scaler.transform(test_multi_features[['token_length', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', 'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', 'google frequency1', 'google frequency2', 'familarity', 'token_vowels']])\n",
        "\n",
        "\n",
        "test_multi_features.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>corpus</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>syllables</th>\n",
              "      <th>pos1</th>\n",
              "      <th>dep num1</th>\n",
              "      <th>pos2</th>\n",
              "      <th>dep num2</th>\n",
              "      <th>synonyms1</th>\n",
              "      <th>hypernyms1</th>\n",
              "      <th>hyponyms1</th>\n",
              "      <th>synonyms2</th>\n",
              "      <th>hypernyms2</th>\n",
              "      <th>hyponyms2</th>\n",
              "      <th>google frequency1</th>\n",
              "      <th>google frequency2</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>biomedical</th>\n",
              "      <th>bible</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>wiki</th>\n",
              "      <th>familarity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL</th>\n",
              "      <td>come intending bring bound chief priest</td>\n",
              "      <td>bible</td>\n",
              "      <td>chief priest</td>\n",
              "      <td>0.812903</td>\n",
              "      <td>-0.505404</td>\n",
              "      <td>0.277033</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.200765</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>0.217289</td>\n",
              "      <td>-0.582030</td>\n",
              "      <td>3.160928</td>\n",
              "      <td>0.209791</td>\n",
              "      <td>-0.519379</td>\n",
              "      <td>-0.342350</td>\n",
              "      <td>0.393219</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.957911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302U8RURJZ1WF35NXY44RD66WL4NVH</th>\n",
              "      <td>day lord take away beauty anklet headband cres...</td>\n",
              "      <td>bible</td>\n",
              "      <td>crescent necklace</td>\n",
              "      <td>1.962864</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>0.098326</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>0.633561</td>\n",
              "      <td>-0.553188</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>-0.754824</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.211593</td>\n",
              "      <td>-0.493225</td>\n",
              "      <td>-0.553577</td>\n",
              "      <td>0.934757</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.121629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3UDTAB6HH6ZVX00DTRXAOJLWX0B094</th>\n",
              "      <td>unclean shall take ash burning sin offering ru...</td>\n",
              "      <td>bible</td>\n",
              "      <td>sin offering</td>\n",
              "      <td>0.812903</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>1.709990</td>\n",
              "      <td>2.390326</td>\n",
              "      <td>-0.098772</td>\n",
              "      <td>0.504082</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>-0.278444</td>\n",
              "      <td>1.837089</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.369612</td>\n",
              "      <td>-0.238985</td>\n",
              "      <td>0.113318</td>\n",
              "      <td>0.393219</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.053560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3L2OEKSTW9ASGQDOW725GFK5P77Y8D</th>\n",
              "      <td>precious treasure oil dwelling wise foolish ma...</td>\n",
              "      <td>bible</td>\n",
              "      <td>precious treasure</td>\n",
              "      <td>1.962864</td>\n",
              "      <td>0.694260</td>\n",
              "      <td>0.277033</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>-0.098772</td>\n",
              "      <td>-0.024553</td>\n",
              "      <td>-2.087483</td>\n",
              "      <td>-0.333525</td>\n",
              "      <td>0.109147</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.158920</td>\n",
              "      <td>-0.393608</td>\n",
              "      <td>-0.489569</td>\n",
              "      <td>2.559373</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.076898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39N6W9XWRDN795J6F5ET8S13DQKYGT</th>\n",
              "      <td>long god shall adversary reproach</td>\n",
              "      <td>bible</td>\n",
              "      <td>adversary reproach</td>\n",
              "      <td>2.192856</td>\n",
              "      <td>1.893924</td>\n",
              "      <td>-0.322563</td>\n",
              "      <td>-0.707506</td>\n",
              "      <td>-0.248693</td>\n",
              "      <td>2.098228</td>\n",
              "      <td>-0.729400</td>\n",
              "      <td>0.336733</td>\n",
              "      <td>-0.113199</td>\n",
              "      <td>-0.409235</td>\n",
              "      <td>0.195761</td>\n",
              "      <td>-0.264266</td>\n",
              "      <td>-0.489295</td>\n",
              "      <td>-0.547988</td>\n",
              "      <td>1.476296</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.920806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         sentence  ... familarity\n",
              "id                                                                                 ...           \n",
              "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL            come intending bring bound chief priest  ...   0.957911\n",
              "302U8RURJZ1WF35NXY44RD66WL4NVH  day lord take away beauty anklet headband cres...  ...   0.121629\n",
              "3UDTAB6HH6ZVX00DTRXAOJLWX0B094  unclean shall take ash burning sin offering ru...  ...   0.053560\n",
              "3L2OEKSTW9ASGQDOW725GFK5P77Y8D  precious treasure oil dwelling wise foolish ma...  ...   0.076898\n",
              "39N6W9XWRDN795J6F5ET8S13DQKYGT                  long god shall adversary reproach  ...  -0.920806\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Qz1Qcm7ACT",
        "outputId": "c95f17bd-0bca-4e91-9a58-ca031bf7a468"
      },
      "source": [
        "sentences_test_list = list(test_multi_features['sentence'])\n",
        "test_tokens_list = list(test_multi_features['token'])\n",
        "\n",
        "test_vectors = get_embeddings(sentences_test_list, test_tokens_list)\n",
        "print(test_vectors.shape)\n",
        "\n",
        "test_f_vectors = test_multi_features[['token_length', 'token_vowels', 'syllables', 'pos1', 'pos2', 'dep num1', 'dep num2', \n",
        "                        'synonyms1', 'synonyms2', 'hypernyms1', 'hypernyms2', 'hyponyms1', 'hyponyms2', \n",
        "                        'google frequency1', 'google frequency2', \n",
        "                        'biomedical', 'bible', 'subtitles', 'wiki', 'familarity']].values\n",
        "test_vectors = np.concatenate((test_vectors, test_f_vectors), axis=1)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(184, 300)\n",
            "(184, 320)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTuZSPMg7C7-"
      },
      "source": [
        "reg = LinearRegression().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/linear_regression_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# Gradient Boosting\n",
        "reg = GradientBoostingRegressor(n_estimators=100).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/gradient_boosting_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# AdaBoost\n",
        "reg = AdaBoostRegressor().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/ada_boost_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# SVM regressor\n",
        "reg = SVR().fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/SVM_baseline.csv\", index=False, header=False)\n",
        "\n",
        "# MLP Regressor\n",
        "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(vectors, np.array(complexity_train_list))\n",
        "y_pred = reg.predict(test_vectors)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":test_multi_features.index, \"complexity\":y_pred})\n",
        "pred.to_csv(SUBMISSION_FOLDER+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgKBP9-77GLe",
        "outputId": "8d2a5506-571b-45ee-f0ab-33730749e492"
      },
      "source": [
        "evaluate(SUBMISSION_FOLDER, FOLDER_PATH+\"/references/lcp_multi_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file linear_regression_baseline.csv\n",
            "pearson  :  0.730243938549043\n",
            "spearman :  0.6997649142287377\n",
            "mae      :  0.08612029690470382\n",
            "mse      :  0.011363087082887057\n",
            "r2       :  0.52922795387472\n",
            "\n",
            "For file gradient_boosting_baseline.csv\n",
            "pearson  :  0.7775428358026494\n",
            "spearman :  0.7475872283891783\n",
            "mae      :  0.08626575847804864\n",
            "mse      :  0.011010708764663023\n",
            "r2       :  0.5438269673884285\n",
            "\n",
            "For file ada_boost_baseline.csv\n",
            "pearson  :  0.7648474859163565\n",
            "spearman :  0.7502571614605091\n",
            "mae      :  0.08956356841439955\n",
            "mse      :  0.012417509570243967\n",
            "r2       :  0.48554329069889246\n",
            "\n",
            "For file SVM_baseline.csv\n",
            "pearson  :  0.7210195529865164\n",
            "spearman :  0.715618549910612\n",
            "mae      :  0.1545108952206832\n",
            "mse      :  0.03618523973084165\n",
            "r2       :  -0.4991524066797841\n",
            "\n",
            "For file MLP_baseline.csv\n",
            "pearson  :  0.24488732175520614\n",
            "spearman :  0.21205995707562106\n",
            "mae      :  0.29056876904625023\n",
            "mse      :  0.12700353282624\n",
            "r2       :  -4.261749080827882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "GwSwGxKH7IpY",
        "outputId": "c979d183-34b8-4305-e662-90ff30318762"
      },
      "source": [
        "'''\n",
        "For file gradient_boosting_baseline.csv\n",
        "pearson  :  0.7775428358026494\n",
        "spearman :  0.7475872283891783\n",
        "mae      :  0.08626575847804864\n",
        "mse      :  0.011010708764663023\n",
        "r2       :  0.5438269673884285\n",
        "'''"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nFor file gradient_boosting_baseline.csv\\npearson  :  0.7775428358026494\\nspearman :  0.7475872283891783\\nmae      :  0.08626575847804864\\nmse      :  0.011010708764663023\\nr2       :  0.5438269673884285\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoHCl17M7aik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}