{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlpTask1",
      "language": "python",
      "name": "nlptask1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcp1W7o_knOE",
        "outputId": "97fd9514-f6bf-4b07-f80c-3c72efb265be"
      },
      "source": [
        "#only run it when you are on colab\r\n",
        "!pip install syllables\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "%cd /content/drive/MyDrive/nlp_project\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syllables\n",
            "  Downloading https://files.pythonhosted.org/packages/16/d9/81a31f640ccf405fdfd0eae8eebfc2579b438804dbf34dc03cad3e76169a/syllables-0.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: syllables\n",
            "Successfully installed syllables-0.1.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g4wjwB-w8S2JDBYO8QPmCkklvoLlrl2chrVyGP_uQmQrT9dzMkgH5s\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/nlp_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EKBTxPQ1wmc"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "import syllables \n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from eval import evaluate"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17rFnJ6K1xK8",
        "outputId": "9ff38e98-01d7-431e-9415-4acdbb5aa8a2"
      },
      "source": [
        "word_vector = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 100.0% 1662.2/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9jJLUYF10kL",
        "outputId": "f0e2ac1d-5703-4168-bcc0-fb9ad4e2e198"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dey6diD6ubIG"
      },
      "source": [
        "# **add feature on single word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HWw1xv1HsKS"
      },
      "source": [
        "def add_single_features(name,isTrain) :\n",
        "    df = pd.read_csv(\"data/preprocessed/\" +name )\n",
        "\n",
        "    frequency_of_word = {}\n",
        "    for i in range(len(df)) :\n",
        "        words = df.at[i,\"sentence\"].split()\n",
        "        for word in words :\n",
        "          if word in frequency_of_word :\n",
        "              frequency_of_word[word]+=1\n",
        "          else :\n",
        "              frequency_of_word[word] = 1\n",
        "\n",
        "    def vowel_count(str): \n",
        "        count = 0\n",
        "        vowel = set(\"aeiouAEIOU\") \n",
        "        for alphabet in str: \n",
        "            if alphabet in vowel: \n",
        "                  count = count + 1\n",
        "        return count \n",
        "\n",
        "\n",
        "    def contain_numeral(str) :\n",
        "        num = \"0123456789\"\n",
        "        for alphabet in str: \n",
        "            if alphabet in num: \n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    token_length=[]\n",
        "    token_syllable= []\n",
        "    token_vowels = []\n",
        "    token_frequency = []\n",
        "    token_contain_numeral = []\n",
        "    nearest_class = []\n",
        "    \n",
        "    for i in range(len(df)) :\n",
        "        word = df.at[i,\"token\"]\n",
        "        # print(word)\n",
        "        if type(word) is not str :\n",
        "            token_length.append(0)\n",
        "            token_syllable.append(0)\n",
        "            token_frequency.append(0)\n",
        "            token_vowels.append(0)\n",
        "            token_contain_numeral.append(False)\n",
        "            if isTrain :\n",
        "                nearest_class.append(round(4*df.at[i,\"complexity\"])+1)   \n",
        "\n",
        "            continue\n",
        "        token_length.append(len(word))\n",
        "        token_syllable.append(syllables.estimate(word))\n",
        "        token_vowels.append(vowel_count(word))\n",
        "        if word in frequency_of_word :\n",
        "          token_frequency.append(frequency_of_word[word])\n",
        "        else :\n",
        "          token_frequency.append(0)\n",
        "        token_contain_numeral.append(contain_numeral(word))\n",
        "\n",
        "        # print(type(df.at[i,\"complexity\"]))\n",
        "        if isTrain :\n",
        "            nearest_class.append(round(4*df.at[i,\"complexity\"])+1)\n",
        "\n",
        "    df['token_length'] = token_length\n",
        "    df['token_syllable'] = token_syllable\n",
        "    df['token_vowels'] = token_vowels\n",
        "    df['token_frequency'] = token_frequency\n",
        "    df['token_contain_numeral'] = token_contain_numeral\n",
        "    if isTrain :\n",
        "        df['nearest_class'] = nearest_class\n",
        "    df.to_csv(\"data/added_features/\"+name,index=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQh43VwhJjx0"
      },
      "source": [
        "add_single_features(\"lcp_single_test_preprocessed.csv\",False)\n",
        "add_single_features(\"lcp_single_train_preprocessed.csv\",True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygNjyq2Um8PO"
      },
      "source": [
        "# **Models on single word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "tglUvpkDPzzS",
        "outputId": "bcb0630a-b77b-4462-9f1e-c4d4fc1d8574"
      },
      "source": [
        "data_folder = \"data/added_features\"\n",
        "df_Train = pd.read_csv(os.path.join(data_folder,\"lcp_single_train_preprocessed.csv\"),index_col=0)\n",
        "df_Test = pd.read_csv(os.path.join(data_folder,\"lcp_single_test_preprocessed.csv\"),index_col=0)\n",
        "df_Test.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>token_syllable</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>token_frequency</th>\n",
              "      <th>token_contain_numeral</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39TX062QX1OHFOH8FUL76K5L7D3X3S</th>\n",
              "      <td>bible</td>\n",
              "      <td>speak much prince world come nothing</td>\n",
              "      <td>prince</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3CIS7GGG65JS8I3AZ9RG54AE4MUUEA</th>\n",
              "      <td>bible</td>\n",
              "      <td>house shall turned others field wife together ...</td>\n",
              "      <td>inhabitant</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379OL9DBSSESUVWY1Z8JGBFG9BTY92</th>\n",
              "      <td>bible</td>\n",
              "      <td>stranger terrible nation cut left mountain val...</td>\n",
              "      <td>bough</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3DFYDSXB2W00JYP2DA272KN69UQUJV</th>\n",
              "      <td>bible</td>\n",
              "      <td>sharpen tongue like sword aim arrow deadly word</td>\n",
              "      <td>arrow</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31YWE12TE0CZG7IVH6OXJ1H1CFPX7X</th>\n",
              "      <td>bible</td>\n",
              "      <td>obey leader submit watch behalf soul give acco...</td>\n",
              "      <td>account</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               corpus  ... token_contain_numeral\n",
              "id                                     ...                      \n",
              "39TX062QX1OHFOH8FUL76K5L7D3X3S  bible  ...                 False\n",
              "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA  bible  ...                 False\n",
              "379OL9DBSSESUVWY1Z8JGBFG9BTY92  bible  ...                 False\n",
              "3DFYDSXB2W00JYP2DA272KN69UQUJV  bible  ...                 False\n",
              "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X  bible  ...                 False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR0rYPVNfW0g",
        "outputId": "d25bea95-c9e4-4220-88c1-37245fc93478"
      },
      "source": [
        "stat_feature = np.array(df_Train[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\n",
        "\n",
        "embed_word = np.array(list(df_Train['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))\n",
        "embed_sentence = np.array(list(df_Train['sentence'].apply\n",
        "    (\n",
        "    lambda x:\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
        "    )\n",
        "                              )\n",
        "                         )\n",
        "Train_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence))\n",
        "Train_Vector.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7662, 305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvNsj9g5q1KM",
        "outputId": "b83f8987-922c-41c0-8f79-93eb126a8c57"
      },
      "source": [
        "stat_feature = np.array(df_Test[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\n",
        "\n",
        "embed_word = np.array(list(df_Test['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))\n",
        "embed_sentence = np.array(list(df_Test['sentence'].apply\n",
        "    (\n",
        "    lambda x:\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
        "    )\n",
        "                              )\n",
        "                         )\n",
        "Test_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence)) # change lambda1 and lambda2\n",
        "Test_Vector.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(917, 305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQEjt5cPvjcc"
      },
      "source": [
        "submission_folder = \"predictions/single_with_features\""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRFS8Mvlv_Tc"
      },
      "source": [
        "# Linear Regression\n",
        "reg = LinearRegression().fit(Train_Vector, np.array(df_Train['complexity']))\n",
        "y_pred = reg.predict(Test_Vector)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
        "pred.to_csv(submission_folder+\"/linear_regression_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L30VeUnexNzQ"
      },
      "source": [
        "# SVM regressor\n",
        "reg = SVR().fit(Train_Vector, np.array(df_Train['complexity']))\n",
        "y_pred = reg.predict(Test_Vector)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
        "pred.to_csv(submission_folder+\"/SVM_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Ionk_jxf8C"
      },
      "source": [
        "# Gradient Boosting\n",
        "reg = GradientBoostingRegressor().fit(Train_Vector, np.array(df_Train['complexity']))\n",
        "y_pred = reg.predict(Test_Vector)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
        "pred.to_csv(submission_folder+\"/gradient_boosting_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcqtLvUTxrF6"
      },
      "source": [
        "# MLP Regressor\n",
        "regr = MLPRegressor(hidden_layer_sizes=(150)).fit(Train_Vector, np.array(df_Train['complexity']))\n",
        "y_pred = reg.predict(Test_Vector)\n",
        "\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
        "pred.to_csv(submission_folder+\"/MLP_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyzblkzgj888"
      },
      "source": [
        "### evaluate baseline models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb3PIE5RyMO1",
        "outputId": "6b62225b-abe7-4bfd-8952-71fb01061057"
      },
      "source": [
        "evaluate(submission_folder, \"references/lcp_single_test_labelled_preprocessed.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For file MLP_with_features.csv\n",
            "pearson  :  0.6510099362950927\n",
            "spearman :  0.6197440617933732\n",
            "mae      :  0.07254862857837216\n",
            "mse      :  0.009343434124671623\n",
            "r2       :  0.4227129699873945\n",
            "\n",
            "For file linear_regression_with_features.csv\n",
            "pearson  :  0.6311508461209863\n",
            "spearman :  0.6077421054566633\n",
            "mae      :  0.07661968054084875\n",
            "mse      :  0.009917703426107611\n",
            "r2       :  0.3872315597232592\n",
            "\n",
            "For file gradient_boosting_with_features.csv\n",
            "pearson  :  0.6510099362950927\n",
            "spearman :  0.6197440617933732\n",
            "mae      :  0.07254862857837216\n",
            "mse      :  0.009343434124671623\n",
            "r2       :  0.4227129699873945\n",
            "\n",
            "For file SVM_with_features.csv\n",
            "pearson  :  0.6744168783485909\n",
            "spearman :  0.6307686935582127\n",
            "mae      :  0.07273162781364834\n",
            "mse      :  0.008875871431429068\n",
            "r2       :  0.4516014787439301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A83rwQFqljsi"
      },
      "source": [
        "## ***add features on multi-word (2 - Word)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BeUamKIj88_"
      },
      "source": [
        "def add_multiword_features(name,isTrain) :\r\n",
        "    df = pd.read_csv(\"data/preprocessed/\" +name )\r\n",
        "\r\n",
        "    frequency_of_word = {}\r\n",
        "    for i in range(len(df)) :\r\n",
        "        words = df.at[i,\"sentence\"].split()\r\n",
        "        for word in words :\r\n",
        "          if word in frequency_of_word :\r\n",
        "              frequency_of_word[word]+=1\r\n",
        "          else :\r\n",
        "              frequency_of_word[word] = 1\r\n",
        "\r\n",
        "    def vowel_count(str): \r\n",
        "        count = 0\r\n",
        "        vowel = set(\"aeiouAEIOU\") \r\n",
        "        for alphabet in str: \r\n",
        "            if alphabet in vowel: \r\n",
        "                  count = count + 1\r\n",
        "        return count \r\n",
        "\r\n",
        "\r\n",
        "    def contain_numeral(str) :\r\n",
        "        num = \"0123456789\"\r\n",
        "        for alphabet in str: \r\n",
        "            if alphabet in num: \r\n",
        "                return True\r\n",
        "        return False\r\n",
        "\r\n",
        "    token_length=[]\r\n",
        "    token_syllable= []\r\n",
        "    token_vowels = []\r\n",
        "    token_frequency = []\r\n",
        "    token_contain_numeral = []\r\n",
        "    nearest_class = []\r\n",
        "    \r\n",
        "    for i in range(len(df)) :\r\n",
        "        word1,word2 = df.at[i,\"token\"].split()\r\n",
        "        # print(word1,word2)\r\n",
        "        if (type(word1) is not str) or (type(word2) is not str) :\r\n",
        "            token_length.append(0)\r\n",
        "            token_syllable.append(0)\r\n",
        "            token_frequency.append(0)\r\n",
        "            token_vowels.append(0)\r\n",
        "            token_contain_numeral.append(False)\r\n",
        "            if isTrain :\r\n",
        "                nearest_class.append(round(4*df.at[i,\"complexity\"])+1)   \r\n",
        "\r\n",
        "            continue\r\n",
        "        token_length.append(len(word1)+len(word2))\r\n",
        "        token_syllable.append(syllables.estimate(word1)+syllables.estimate(word2))\r\n",
        "        token_vowels.append(vowel_count(word1)+vowel_count(word2))\r\n",
        "        token_frequency.append(frequency_of_word[word1]+frequency_of_word[word2])\r\n",
        "        token_contain_numeral.append(contain_numeral(word1) or contain_numeral(word2))\r\n",
        "\r\n",
        "        # print(type(df.at[i,\"complexity\"]))\r\n",
        "        if isTrain :\r\n",
        "            nearest_class.append(round(4*df.at[i,\"complexity\"])+1)\r\n",
        "\r\n",
        "    df['token_length'] = token_length\r\n",
        "    df['token_syllable'] = token_syllable\r\n",
        "    df['token_vowels'] = token_vowels\r\n",
        "    df['token_frequency'] = token_frequency\r\n",
        "    df['token_contain_numeral'] = token_contain_numeral\r\n",
        "    if isTrain :\r\n",
        "        df['nearest_class'] = nearest_class\r\n",
        "    df.to_csv(\"data/added_features/\"+name,index=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8VG8qZAm5tu"
      },
      "source": [
        "add_multiword_features(\"lcp_multi_test_preprocessed.csv\",False)\r\n",
        "add_multiword_features(\"lcp_multi_train_preprocessed.csv\",True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1OC2pLKsXeV"
      },
      "source": [
        "## ***Models on multi-word (2 - Word)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "KAHJbSXinauw",
        "outputId": "b0ed6be3-4ad7-4048-f855-900b86f37c2b"
      },
      "source": [
        "data_folder = \"data/added_features\"\r\n",
        "df_Train = pd.read_csv(os.path.join(data_folder,\"lcp_multi_train_preprocessed.csv\"),index_col=0)\r\n",
        "df_Test = pd.read_csv(os.path.join(data_folder,\"lcp_multi_test_preprocessed.csv\"),index_col=0)\r\n",
        "df_Test.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>token_syllable</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>token_frequency</th>\n",
              "      <th>token_contain_numeral</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL</th>\n",
              "      <td>bible</td>\n",
              "      <td>come intending bring bound chief priest</td>\n",
              "      <td>chief priest</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302U8RURJZ1WF35NXY44RD66WL4NVH</th>\n",
              "      <td>bible</td>\n",
              "      <td>day lord take away beauty anklet headband cres...</td>\n",
              "      <td>crescent necklace</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3UDTAB6HH6ZVX00DTRXAOJLWX0B094</th>\n",
              "      <td>bible</td>\n",
              "      <td>unclean shall take ash burning sin offering ru...</td>\n",
              "      <td>sin offering</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3L2OEKSTW9ASGQDOW725GFK5P77Y8D</th>\n",
              "      <td>bible</td>\n",
              "      <td>precious treasure oil dwelling wise foolish ma...</td>\n",
              "      <td>precious treasure</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39N6W9XWRDN795J6F5ET8S13DQKYGT</th>\n",
              "      <td>bible</td>\n",
              "      <td>long god shall adversary reproach</td>\n",
              "      <td>adversary reproach</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               corpus  ... token_contain_numeral\n",
              "id                                     ...                      \n",
              "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL  bible  ...                 False\n",
              "302U8RURJZ1WF35NXY44RD66WL4NVH  bible  ...                 False\n",
              "3UDTAB6HH6ZVX00DTRXAOJLWX0B094  bible  ...                 False\n",
              "3L2OEKSTW9ASGQDOW725GFK5P77Y8D  bible  ...                 False\n",
              "39N6W9XWRDN795J6F5ET8S13DQKYGT  bible  ...                 False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNXggBuSsjDw",
        "outputId": "3a50109b-d208-423a-9f96-59ace49e83c9"
      },
      "source": [
        "stat_feature = np.array(df_Train[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\r\n",
        "\r\n",
        "\r\n",
        "embed_word = np.array(list(df_Train['token'].apply\r\n",
        "    (\r\n",
        "    lambda x:\r\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\r\n",
        "    )\r\n",
        "                              )\r\n",
        "                         )\r\n",
        "embed_sentence = np.array(list(df_Train['sentence'].apply\r\n",
        "    (\r\n",
        "    lambda x:\r\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\r\n",
        "    )\r\n",
        "                              )\r\n",
        "                         )\r\n",
        "Train_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence))\r\n",
        "Train_Vector.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1517, 305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmT2-JdZtI5U",
        "outputId": "f8926352-933f-4203-b664-d8702597f581"
      },
      "source": [
        "stat_feature = np.array(df_Test[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\r\n",
        "\r\n",
        "embed_word = np.array(list(df_Test['token'].apply\r\n",
        "    (\r\n",
        "    lambda x:\r\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\r\n",
        "    )\r\n",
        "                              )\r\n",
        "                         )\r\n",
        "embed_sentence = np.array(list(df_Test['sentence'].apply\r\n",
        "    (\r\n",
        "    lambda x:\r\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\r\n",
        "    )\r\n",
        "                              )\r\n",
        "                         )\r\n",
        "Test_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence)) # change lambda1 and lambda2\r\n",
        "Test_Vector.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(184, 305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHGxsii7tinq"
      },
      "source": [
        "submission_folder = \"predictions/multiword_with_features\""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAr5I_JZtpKk"
      },
      "source": [
        "# Linear Regression\r\n",
        "reg = LinearRegression().fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/linear_regression_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ult9me-mtvPV"
      },
      "source": [
        "# SVM regressor\r\n",
        "reg = SVR().fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/SVM_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZQenSWtzSw"
      },
      "source": [
        "# Gradient Boosting\r\n",
        "reg = GradientBoostingRegressor().fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/gradient_boosting_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNqPHoJ2t2Lb"
      },
      "source": [
        "# MLP Regressor\r\n",
        "regr = MLPRegressor(hidden_layer_sizes=(150)).fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/MLP_with_features.csv\", index=False, header=False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flggadnpt48v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}