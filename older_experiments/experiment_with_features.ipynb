{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gcp1W7o_knOE",
    "outputId": "97fd9514-f6bf-4b07-f80c-3c72efb265be"
   },
   "outputs": [],
   "source": [
    "# #only run it when you are on colab\n",
    "# !pip install syllables\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/MyDrive/nlp_project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3EKBTxPQ1wmc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import syllables \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from eval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17rFnJ6K1xK8",
    "outputId": "9ff38e98-01d7-431e-9415-4acdbb5aa8a2"
   },
   "outputs": [],
   "source": [
    "word_vector = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9jJLUYF10kL",
    "outputId": "f0e2ac1d-5703-4168-bcc0-fb9ad4e2e198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ankit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dey6diD6ubIG"
   },
   "source": [
    "# **add feature on single word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1HWw1xv1HsKS"
   },
   "outputs": [],
   "source": [
    "def add_single_features(name,isTrain) :\n",
    "    df = pd.read_csv(\"data/preprocessed/\" +name )\n",
    "\n",
    "    frequency_of_word = {}\n",
    "    for i in range(len(df)) :\n",
    "        word = df.at[i,\"token\"]\n",
    "        if word in frequency_of_word :\n",
    "            frequency_of_word[word]+=1\n",
    "        else :\n",
    "            frequency_of_word[word] = 1\n",
    "\n",
    "    def vowel_count(str): \n",
    "        count = 0\n",
    "        vowel = set(\"aeiouAEIOU\") \n",
    "        for alphabet in str: \n",
    "            if alphabet in vowel: \n",
    "                  count = count + 1\n",
    "        return count \n",
    "\n",
    "\n",
    "    def contain_numeral(str) :\n",
    "        num = \"0123456789\"\n",
    "        for alphabet in str: \n",
    "            if alphabet in num: \n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    token_length=[]\n",
    "    token_syllable= []\n",
    "    token_vowels = []\n",
    "    token_frequency = []\n",
    "    token_contain_numeral = []\n",
    "    nearest_class = []\n",
    "    \n",
    "    for i in range(len(df)) :\n",
    "        word = df.at[i,\"token\"]\n",
    "        # print(word)\n",
    "        if type(word) is not str :\n",
    "            token_length.append(0)\n",
    "            token_syllable.append(0)\n",
    "            token_frequency.append(0)\n",
    "            token_vowels.append(0)\n",
    "            token_contain_numeral.append(False)\n",
    "            if isTrain :\n",
    "                nearest_class.append(round(4*df.at[i,\"complexity\"])+1)   \n",
    "\n",
    "            continue\n",
    "        token_length.append(len(word))\n",
    "        token_syllable.append(syllables.estimate(word))\n",
    "        token_vowels.append(vowel_count(word))\n",
    "        if word in frequency_of_word :\n",
    "          token_frequency.append(frequency_of_word[word])\n",
    "        else :\n",
    "          token_frequency.append(0)\n",
    "        token_contain_numeral.append(contain_numeral(word))\n",
    "\n",
    "        # print(type(df.at[i,\"complexity\"]))\n",
    "        if isTrain :\n",
    "            nearest_class.append(round(4*df.at[i,\"complexity\"])+1)\n",
    "\n",
    "    df['token_length'] = token_length\n",
    "    df['token_syllable'] = token_syllable\n",
    "    df['token_vowels'] = token_vowels\n",
    "    df['token_frequency'] = token_frequency\n",
    "    df['token_contain_numeral'] = token_contain_numeral\n",
    "    if isTrain :\n",
    "        df['nearest_class'] = nearest_class\n",
    "    df.to_csv(\"data/added_features/\"+name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zQh43VwhJjx0"
   },
   "outputs": [],
   "source": [
    "add_single_features(\"lcp_single_test_preprocessed.csv\",False)\n",
    "add_single_features(\"lcp_single_train_preprocessed.csv\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygNjyq2Um8PO"
   },
   "source": [
    "# **Models on single word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "tglUvpkDPzzS",
    "outputId": "bcb0630a-b77b-4462-9f1e-c4d4fc1d8574"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token_syllable</th>\n",
       "      <th>token_vowels</th>\n",
       "      <th>token_frequency</th>\n",
       "      <th>token_contain_numeral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39TX062QX1OHFOH8FUL76K5L7D3X3S</th>\n",
       "      <td>bible</td>\n",
       "      <td>speak much prince world come nothing</td>\n",
       "      <td>prince</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3CIS7GGG65JS8I3AZ9RG54AE4MUUEA</th>\n",
       "      <td>bible</td>\n",
       "      <td>house shall turned others field wife together ...</td>\n",
       "      <td>inhabitant</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379OL9DBSSESUVWY1Z8JGBFG9BTY92</th>\n",
       "      <td>bible</td>\n",
       "      <td>stranger terrible nation cut left mountain val...</td>\n",
       "      <td>bough</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3DFYDSXB2W00JYP2DA272KN69UQUJV</th>\n",
       "      <td>bible</td>\n",
       "      <td>sharpen tongue like sword aim arrow deadly word</td>\n",
       "      <td>arrow</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31YWE12TE0CZG7IVH6OXJ1H1CFPX7X</th>\n",
       "      <td>bible</td>\n",
       "      <td>obey leader submit watch behalf soul give acco...</td>\n",
       "      <td>account</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               corpus  \\\n",
       "id                                      \n",
       "39TX062QX1OHFOH8FUL76K5L7D3X3S  bible   \n",
       "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA  bible   \n",
       "379OL9DBSSESUVWY1Z8JGBFG9BTY92  bible   \n",
       "3DFYDSXB2W00JYP2DA272KN69UQUJV  bible   \n",
       "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X  bible   \n",
       "\n",
       "                                                                         sentence  \\\n",
       "id                                                                                  \n",
       "39TX062QX1OHFOH8FUL76K5L7D3X3S               speak much prince world come nothing   \n",
       "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA  house shall turned others field wife together ...   \n",
       "379OL9DBSSESUVWY1Z8JGBFG9BTY92  stranger terrible nation cut left mountain val...   \n",
       "3DFYDSXB2W00JYP2DA272KN69UQUJV    sharpen tongue like sword aim arrow deadly word   \n",
       "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X  obey leader submit watch behalf soul give acco...   \n",
       "\n",
       "                                     token  token_length  token_syllable  \\\n",
       "id                                                                         \n",
       "39TX062QX1OHFOH8FUL76K5L7D3X3S      prince             6               2   \n",
       "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA  inhabitant            10               4   \n",
       "379OL9DBSSESUVWY1Z8JGBFG9BTY92       bough             5               1   \n",
       "3DFYDSXB2W00JYP2DA272KN69UQUJV       arrow             5               2   \n",
       "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X     account             7               2   \n",
       "\n",
       "                                token_vowels  token_frequency  \\\n",
       "id                                                              \n",
       "39TX062QX1OHFOH8FUL76K5L7D3X3S             2                5   \n",
       "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA             4                3   \n",
       "379OL9DBSSESUVWY1Z8JGBFG9BTY92             2                2   \n",
       "3DFYDSXB2W00JYP2DA272KN69UQUJV             2                3   \n",
       "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X             3                4   \n",
       "\n",
       "                                token_contain_numeral  \n",
       "id                                                     \n",
       "39TX062QX1OHFOH8FUL76K5L7D3X3S                  False  \n",
       "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA                  False  \n",
       "379OL9DBSSESUVWY1Z8JGBFG9BTY92                  False  \n",
       "3DFYDSXB2W00JYP2DA272KN69UQUJV                  False  \n",
       "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X                  False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = \"data/added_features\"\n",
    "df_Train = pd.read_csv(os.path.join(data_folder,\"lcp_single_train_preprocessed.csv\"),index_col=0)\n",
    "df_Test = pd.read_csv(os.path.join(data_folder,\"lcp_single_test_preprocessed.csv\"),index_col=0)\n",
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR0rYPVNfW0g",
    "outputId": "d25bea95-c9e4-4220-88c1-37245fc93478"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7662, 305)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_feature = np.array(df_Train[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\n",
    "\n",
    "embed_word = np.array(list(df_Train['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))\n",
    "embed_sentence = np.array(list(df_Train['sentence'].apply\n",
    "    (\n",
    "    lambda x:\n",
    "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
    "    )\n",
    "                              )\n",
    "                         )\n",
    "Train_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence))\n",
    "Train_Vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvNsj9g5q1KM",
    "outputId": "b83f8987-922c-41c0-8f79-93eb126a8c57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917, 305)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_feature = np.array(df_Test[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\n",
    "\n",
    "embed_word = np.array(list(df_Test['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))\n",
    "embed_sentence = np.array(list(df_Test['sentence'].apply\n",
    "    (\n",
    "    lambda x:\n",
    "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
    "    )\n",
    "                              )\n",
    "                         )\n",
    "Test_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence)) # change lambda1 and lambda2\n",
    "Test_Vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XQEjt5cPvjcc"
   },
   "outputs": [],
   "source": [
    "submission_folder = \"predictions/single_with_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pRFS8Mvlv_Tc"
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "reg = LinearRegression().fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/linear_regression_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L30VeUnexNzQ"
   },
   "outputs": [],
   "source": [
    "# SVM regressor\n",
    "reg = SVR().fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/SVM_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "81Ionk_jxf8C"
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "reg = GradientBoostingRegressor().fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/gradient_boosting_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gcqtLvUTxrF6"
   },
   "outputs": [],
   "source": [
    "# MLP Regressor\n",
    "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/MLP_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyzblkzgj888"
   },
   "source": [
    "### evaluate baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Fb3PIE5RyMO1",
    "outputId": "6b62225b-abe7-4bfd-8952-71fb01061057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file MLP_with_features.csv\n",
      "pearson  :  0.6057132793111173\n",
      "spearman :  0.5503812552403398\n",
      "mae      :  0.08905447567691986\n",
      "mse      :  0.013068293427130534\n",
      "r2       :  0.19257136089172333\n",
      "\n",
      "For file linear_regression_with_features.csv\n",
      "pearson  :  0.6311508461209863\n",
      "spearman :  0.6077421054566633\n",
      "mae      :  0.07661968054084875\n",
      "mse      :  0.009917703426107611\n",
      "r2       :  0.3872315597232592\n",
      "\n",
      "For file gradient_boosting_with_features.csv\n",
      "pearson  :  0.6510099362950927\n",
      "spearman :  0.6197440617933732\n",
      "mae      :  0.07254862857837216\n",
      "mse      :  0.009343434124671625\n",
      "r2       :  0.4227129699873944\n",
      "\n",
      "For file SVM_with_features.csv\n",
      "pearson  :  0.6744168783485909\n",
      "spearman :  0.6307686935582127\n",
      "mae      :  0.07273162781364834\n",
      "mse      :  0.008875871431429068\n",
      "r2       :  0.4516014787439301\n"
     ]
    }
   ],
   "source": [
    "evaluate(submission_folder, \"references/lcp_single_test_labelled_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A83rwQFqljsi"
   },
   "source": [
    "## ***add features on multi-word (2 - Word)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7BeUamKIj88_"
   },
   "outputs": [],
   "source": [
    "def add_multiword_features(name,isTrain) :\n",
    "    df = pd.read_csv(\"data/preprocessed/\" +name )\n",
    "\n",
    "    frequency_of_word = {}\n",
    "    for i in range(len(df)) :\n",
    "        words = df.at[i,\"token\"].split()\n",
    "        for word in words :\n",
    "            if word in frequency_of_word :\n",
    "                frequency_of_word[word]+=1\n",
    "            else :\n",
    "                frequency_of_word[word] = 1\n",
    "\n",
    "    def vowel_count(str): \n",
    "        count = 0\n",
    "        vowel = set(\"aeiouAEIOU\") \n",
    "        for alphabet in str: \n",
    "            if alphabet in vowel: \n",
    "                  count = count + 1\n",
    "        return count \n",
    "\n",
    "\n",
    "    def contain_numeral(str) :\n",
    "        num = \"0123456789\"\n",
    "        for alphabet in str: \n",
    "            if alphabet in num: \n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    token_length=[]\n",
    "    token_syllable= []\n",
    "    token_vowels = []\n",
    "    token_frequency = []\n",
    "    token_contain_numeral = []\n",
    "    nearest_class = []\n",
    "    \n",
    "    for i in range(len(df)) :\n",
    "        word1,word2 = df.at[i,\"token\"].split()\n",
    "        # print(word1,word2)\n",
    "        if (type(word1) is not str) or (type(word2) is not str) :\n",
    "            token_length.append(0)\n",
    "            token_syllable.append(0)\n",
    "            token_frequency.append(0)\n",
    "            token_vowels.append(0)\n",
    "            token_contain_numeral.append(False)\n",
    "            if isTrain :\n",
    "                nearest_class.append(round(4*df.at[i,\"complexity\"])+1)   \n",
    "\n",
    "            continue\n",
    "        token_length.append(len(word1)+len(word2))\n",
    "        token_syllable.append(syllables.estimate(word1)+syllables.estimate(word2))\n",
    "        token_vowels.append(vowel_count(word1)+vowel_count(word2))\n",
    "        token_frequency.append(frequency_of_word[word1]+frequency_of_word[word2])\n",
    "        token_contain_numeral.append(contain_numeral(word1) or contain_numeral(word2))\n",
    "\n",
    "        # print(type(df.at[i,\"complexity\"]))\n",
    "        if isTrain :\n",
    "            nearest_class.append(round(4*df.at[i,\"complexity\"])+1)\n",
    "\n",
    "    df['token_length'] = token_length\n",
    "    df['token_syllable'] = token_syllable\n",
    "    df['token_vowels'] = token_vowels\n",
    "    df['token_frequency'] = token_frequency\n",
    "    df['token_contain_numeral'] = token_contain_numeral\n",
    "    if isTrain :\n",
    "        df['nearest_class'] = nearest_class\n",
    "    df.to_csv(\"data/added_features/\"+name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "j8VG8qZAm5tu"
   },
   "outputs": [],
   "source": [
    "add_multiword_features(\"lcp_multi_test_preprocessed.csv\",False)\n",
    "add_multiword_features(\"lcp_multi_train_preprocessed.csv\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1OC2pLKsXeV"
   },
   "source": [
    "## ***Models on multi-word (2 - Word)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "KAHJbSXinauw",
    "outputId": "b0ed6be3-4ad7-4048-f855-900b86f37c2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token_syllable</th>\n",
       "      <th>token_vowels</th>\n",
       "      <th>token_frequency</th>\n",
       "      <th>token_contain_numeral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL</th>\n",
       "      <td>bible</td>\n",
       "      <td>come intending bring bound chief priest</td>\n",
       "      <td>chief priest</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302U8RURJZ1WF35NXY44RD66WL4NVH</th>\n",
       "      <td>bible</td>\n",
       "      <td>day lord take away beauty anklet headband cres...</td>\n",
       "      <td>crescent necklace</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3UDTAB6HH6ZVX00DTRXAOJLWX0B094</th>\n",
       "      <td>bible</td>\n",
       "      <td>unclean shall take ash burning sin offering ru...</td>\n",
       "      <td>sin offering</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3L2OEKSTW9ASGQDOW725GFK5P77Y8D</th>\n",
       "      <td>bible</td>\n",
       "      <td>precious treasure oil dwelling wise foolish ma...</td>\n",
       "      <td>precious treasure</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39N6W9XWRDN795J6F5ET8S13DQKYGT</th>\n",
       "      <td>bible</td>\n",
       "      <td>long god shall adversary reproach</td>\n",
       "      <td>adversary reproach</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               corpus  \\\n",
       "id                                      \n",
       "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL  bible   \n",
       "302U8RURJZ1WF35NXY44RD66WL4NVH  bible   \n",
       "3UDTAB6HH6ZVX00DTRXAOJLWX0B094  bible   \n",
       "3L2OEKSTW9ASGQDOW725GFK5P77Y8D  bible   \n",
       "39N6W9XWRDN795J6F5ET8S13DQKYGT  bible   \n",
       "\n",
       "                                                                         sentence  \\\n",
       "id                                                                                  \n",
       "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL            come intending bring bound chief priest   \n",
       "302U8RURJZ1WF35NXY44RD66WL4NVH  day lord take away beauty anklet headband cres...   \n",
       "3UDTAB6HH6ZVX00DTRXAOJLWX0B094  unclean shall take ash burning sin offering ru...   \n",
       "3L2OEKSTW9ASGQDOW725GFK5P77Y8D  precious treasure oil dwelling wise foolish ma...   \n",
       "39N6W9XWRDN795J6F5ET8S13DQKYGT                  long god shall adversary reproach   \n",
       "\n",
       "                                             token  token_length  \\\n",
       "id                                                                 \n",
       "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL        chief priest            11   \n",
       "302U8RURJZ1WF35NXY44RD66WL4NVH   crescent necklace            16   \n",
       "3UDTAB6HH6ZVX00DTRXAOJLWX0B094        sin offering            11   \n",
       "3L2OEKSTW9ASGQDOW725GFK5P77Y8D   precious treasure            16   \n",
       "39N6W9XWRDN795J6F5ET8S13DQKYGT  adversary reproach            17   \n",
       "\n",
       "                                token_syllable  token_vowels  token_frequency  \\\n",
       "id                                                                              \n",
       "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL               2             4                7   \n",
       "302U8RURJZ1WF35NXY44RD66WL4NVH               5             5                2   \n",
       "3UDTAB6HH6ZVX00DTRXAOJLWX0B094               4             4               10   \n",
       "3L2OEKSTW9ASGQDOW725GFK5P77Y8D               5             8                2   \n",
       "39N6W9XWRDN795J6F5ET8S13DQKYGT               6             6                2   \n",
       "\n",
       "                                token_contain_numeral  \n",
       "id                                                     \n",
       "3A9LA2FRWSEW9WO7UFA9AE6VQK3XHL                  False  \n",
       "302U8RURJZ1WF35NXY44RD66WL4NVH                  False  \n",
       "3UDTAB6HH6ZVX00DTRXAOJLWX0B094                  False  \n",
       "3L2OEKSTW9ASGQDOW725GFK5P77Y8D                  False  \n",
       "39N6W9XWRDN795J6F5ET8S13DQKYGT                  False  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = \"data/added_features\"\n",
    "df_Train = pd.read_csv(os.path.join(data_folder,\"lcp_multi_train_preprocessed.csv\"),index_col=0)\n",
    "df_Test = pd.read_csv(os.path.join(data_folder,\"lcp_multi_test_preprocessed.csv\"),index_col=0)\n",
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNXggBuSsjDw",
    "outputId": "3a50109b-d208-423a-9f96-59ace49e83c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1517, 305)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_feature = np.array(df_Train[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\n",
    "\n",
    "\n",
    "embed_word = np.array(list(df_Train['token'].apply\n",
    "    (\n",
    "    lambda x:\n",
    "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
    "    )\n",
    "                              )\n",
    "                         )\n",
    "embed_sentence = np.array(list(df_Train['sentence'].apply\n",
    "    (\n",
    "    lambda x:\n",
    "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
    "    )\n",
    "                              )\n",
    "                         )\n",
    "Train_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence))\n",
    "Train_Vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmT2-JdZtI5U",
    "outputId": "f8926352-933f-4203-b664-d8702597f581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, 305)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_feature = np.array(df_Test[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\n",
    "\n",
    "embed_word = np.array(list(df_Test['token'].apply\n",
    "    (\n",
    "    lambda x:\n",
    "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
    "    )\n",
    "                              )\n",
    "                         )\n",
    "embed_sentence = np.array(list(df_Test['sentence'].apply\n",
    "    (\n",
    "    lambda x:\n",
    "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\n",
    "    )\n",
    "                              )\n",
    "                         )\n",
    "Test_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence)) # change lambda1 and lambda2\n",
    "Test_Vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BHGxsii7tinq"
   },
   "outputs": [],
   "source": [
    "submission_folder = \"predictions/multiword_with_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jAr5I_JZtpKk"
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "reg = LinearRegression().fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/linear_regression_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Ult9me-mtvPV"
   },
   "outputs": [],
   "source": [
    "# SVM regressor\n",
    "reg = SVR().fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/SVM_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tZZQenSWtzSw"
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "reg = GradientBoostingRegressor().fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/gradient_boosting_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MNqPHoJ2t2Lb"
   },
   "outputs": [],
   "source": [
    "# MLP Regressor\n",
    "reg = MLPRegressor(hidden_layer_sizes=(150)).fit(Train_Vector, np.array(df_Train['complexity']))\n",
    "y_pred = reg.predict(Test_Vector)\n",
    "\n",
    "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\n",
    "pred.to_csv(submission_folder+\"/MLP_with_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Flggadnpt48v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file MLP_with_features.csv\n",
      "pearson  :  0.6475028865558714\n",
      "spearman :  0.6342185188430863\n",
      "mae      :  0.09616392954292978\n",
      "mse      :  0.015062593128924379\n",
      "r2       :  0.37595763056893994\n",
      "\n",
      "For file linear_regression_with_features.csv\n",
      "pearson  :  0.6419859078673502\n",
      "spearman :  0.6249874798605937\n",
      "mae      :  0.09300396735257285\n",
      "mse      :  0.014517452349322627\n",
      "r2       :  0.398542781801825\n",
      "\n",
      "For file gradient_boosting_with_features.csv\n",
      "pearson  :  0.7223224401942772\n",
      "spearman :  0.6899679064160567\n",
      "mae      :  0.08551579078634078\n",
      "mse      :  0.012048012220533717\n",
      "r2       :  0.5008515447052295\n",
      "\n",
      "For file SVM_with_features.csv\n",
      "pearson  :  0.6427020682357478\n",
      "spearman :  0.6163305512019623\n",
      "mae      :  0.0962766986144412\n",
      "mse      :  0.014806430533685208\n",
      "r2       :  0.386570432197744\n"
     ]
    }
   ],
   "source": [
    "evaluate(submission_folder, \"references/lcp_multi_test_labelled_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlpTask1",
   "language": "python",
   "name": "nlptask1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
