{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.7 64-bit ('data_science': conda)",
      "metadata": {
        "interpreter": {
          "hash": "45b1ade2b41f16140323b6f7ee0dc7572514bfd039e74f6b12013e185993eb2b"
        }
      }
    },
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hvarS/CS60075-Team28-Task-1/blob/main/experiment_sanket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EKBTxPQ1wmc"
      },
      "source": [
        "import gensim.downloader\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "import nltk\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import os "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17rFnJ6K1xK8",
        "outputId": "3b3d0205-da7e-46df-f44f-e8954997aaca"
      },
      "source": [
        "word_vector = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.3% 1651.2/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI3DLaXmVS3J",
        "outputId": "487008ce-ab1c-49b3-e301-de8480da2c03"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj-NJjI6TVXC",
        "outputId": "f1a7ae98-f2e3-4d72-943d-24919de41cd2"
      },
      "source": [
        "%cd /content/drive/MyDrive/nlp_project"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nlp_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9jJLUYF10kL",
        "outputId": "99fa6dae-cd39-4634-e9e0-343a1a18b63f"
      },
      "source": [
        "nltk.download('punkt')\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npzQxAxUXnFL",
        "outputId": "fb1cb313-9af8-421c-84cd-7743263d8d18"
      },
      "source": [
        " !pip install syllables"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syllables\n",
            "  Downloading https://files.pythonhosted.org/packages/16/d9/81a31f640ccf405fdfd0eae8eebfc2579b438804dbf34dc03cad3e76169a/syllables-0.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: syllables\n",
            "Successfully installed syllables-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HWw1xv1HsKS"
      },
      "source": [
        "def add_single_features(name,isTrain) :\r\n",
        "  df = pd.read_csv(\"data/preprocessed/\" +name )\r\n",
        "\r\n",
        "  frequency_of_word = {}\r\n",
        "  for i in range(len(df)) :\r\n",
        "    word = df.at[i,\"token\"]\r\n",
        "    if word in frequency_of_word :\r\n",
        "      frequency_of_word[word]+=1\r\n",
        "    else :\r\n",
        "      frequency_of_word[word] = 1\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def vowel_count(str): \r\n",
        "    count = 0\r\n",
        "    vowel = set(\"aeiouAEIOU\") \r\n",
        "    for alphabet in str: \r\n",
        "      if alphabet in vowel: \r\n",
        "          count = count + 1\r\n",
        "    return count \r\n",
        "\r\n",
        "\r\n",
        "  def contain_numeral(str) :\r\n",
        "    num = \"0123456789\"\r\n",
        "    for alphabet in str: \r\n",
        "      if alphabet in num: \r\n",
        "          return True\r\n",
        "    return False\r\n",
        "\r\n",
        "\r\n",
        "  token_length=[]\r\n",
        "  token_syllable= []\r\n",
        "  token_vowels = []\r\n",
        "  token_frequency = []\r\n",
        "  token_contain_numeral = []\r\n",
        "  nearest_class = []\r\n",
        "  # print(df.head())\r\n",
        "  for i in range(len(df)) :\r\n",
        "    \r\n",
        "    word = df.at[i,\"token\"]\r\n",
        "    if type(word) is not str :\r\n",
        "      token_length.append(0)\r\n",
        "      token_syllable.append(0)\r\n",
        "      token_frequency.append(0)\r\n",
        "      token_vowels.append(0)\r\n",
        "      token_contain_numeral.append(False)\r\n",
        "      if isTrain :\r\n",
        "        nearest_class.append(round(4*df.at[i,\"complexity\"])+1)   \r\n",
        "      \r\n",
        "      continue\r\n",
        "    token_length.append(len(word))\r\n",
        "    import syllables # pip install syllables\r\n",
        "    token_syllable.append(syllables.estimate(word))\r\n",
        "    token_vowels.append(vowel_count(word))\r\n",
        "    token_frequency.append(frequency_of_word[word])\r\n",
        "    token_contain_numeral.append(contain_numeral(word))\r\n",
        "\r\n",
        "    # print(type(df.at[i,\"complexity\"]))\r\n",
        "    if isTrain :\r\n",
        "      nearest_class.append(round(4*df.at[i,\"complexity\"])+1)\r\n",
        "\r\n",
        "  df['token_length'] = token_length\r\n",
        "  df['token_syllable'] = token_syllable\r\n",
        "  df['token_vowels'] = token_vowels\r\n",
        "  df['token_frequency'] = token_frequency\r\n",
        "  df['token_contain_numeral'] = token_contain_numeral\r\n",
        "  if isTrain :\r\n",
        "    df['nearest_class'] = nearest_class\r\n",
        "  df.to_csv(\"data/added_features/\"+name,index=False)\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQh43VwhJjx0"
      },
      "source": [
        "add_single_features(\"lcp_single_test_preprocessed.csv\",False)\r\n",
        "add_single_features(\"lcp_single_train_preprocessed.csv\",True)\r\n",
        "add_single_features(\"lcp_multi_test_preprocessed.csv\",False)\r\n",
        "add_single_features(\"lcp_multi_train_preprocessed.csv\",True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygNjyq2Um8PO"
      },
      "source": [
        "models on single"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tglUvpkDPzzS",
        "outputId": "b62a9e25-7c54-4b98-e3f9-75470f9faaf7"
      },
      "source": [
        "data_folder = \"data/added_features\"\r\n",
        "df_Train = pd.read_csv(os.path.join(data_folder,\"lcp_single_train_preprocessed.csv\"),index_col=0)\r\n",
        "df_Test = pd.read_csv(os.path.join(data_folder,\"lcp_single_test_preprocessed.csv\"),index_col=0)\r\n",
        "df_Test"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>token_length</th>\n",
              "      <th>token_syllable</th>\n",
              "      <th>token_vowels</th>\n",
              "      <th>token_frequency</th>\n",
              "      <th>token_contain_numeral</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39TX062QX1OHFOH8FUL76K5L7D3X3S</th>\n",
              "      <td>bible</td>\n",
              "      <td>speak much prince world come nothing</td>\n",
              "      <td>prince</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3CIS7GGG65JS8I3AZ9RG54AE4MUUEA</th>\n",
              "      <td>bible</td>\n",
              "      <td>house shall turned others field wife together ...</td>\n",
              "      <td>inhabitant</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379OL9DBSSESUVWY1Z8JGBFG9BTY92</th>\n",
              "      <td>bible</td>\n",
              "      <td>stranger terrible nation cut left mountain val...</td>\n",
              "      <td>bough</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3DFYDSXB2W00JYP2DA272KN69UQUJV</th>\n",
              "      <td>bible</td>\n",
              "      <td>sharpen tongue like sword aim arrow deadly word</td>\n",
              "      <td>arrow</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31YWE12TE0CZG7IVH6OXJ1H1CFPX7X</th>\n",
              "      <td>bible</td>\n",
              "      <td>obey leader submit watch behalf soul give acco...</td>\n",
              "      <td>account</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3B0MCRZMBRUJD4K4R18XPAMJAE2PPW</th>\n",
              "      <td>europarl</td>\n",
              "      <td>accession european community convention concer...</td>\n",
              "      <td>convention</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3IKMEYR0LWVMA6MICBSDXGSR5U2K21</th>\n",
              "      <td>europarl</td>\n",
              "      <td>question oral answer commission stress test eu...</td>\n",
              "      <td>liberal</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37M4O367VJI9ZR58F67RA0N7E885MA</th>\n",
              "      <td>europarl</td>\n",
              "      <td>received legal opinion parliament legal servic...</td>\n",
              "      <td>comparison</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36JW4WBR06KF9AXMUL4N476OMH3FHC</th>\n",
              "      <td>europarl</td>\n",
              "      <td>note comment particular keenness may expect gi...</td>\n",
              "      <td>keenness</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3VJ4PFXFJ37PI5MYJ4PU9LKNJ8BUAW</th>\n",
              "      <td>europarl</td>\n",
              "      <td>next item report iles braghetto behalf committ...</td>\n",
              "      <td>fishery</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>917 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  corpus  ... token_contain_numeral\n",
              "id                                        ...                      \n",
              "39TX062QX1OHFOH8FUL76K5L7D3X3S     bible  ...                 False\n",
              "3CIS7GGG65JS8I3AZ9RG54AE4MUUEA     bible  ...                 False\n",
              "379OL9DBSSESUVWY1Z8JGBFG9BTY92     bible  ...                 False\n",
              "3DFYDSXB2W00JYP2DA272KN69UQUJV     bible  ...                 False\n",
              "31YWE12TE0CZG7IVH6OXJ1H1CFPX7X     bible  ...                 False\n",
              "...                                  ...  ...                   ...\n",
              "3B0MCRZMBRUJD4K4R18XPAMJAE2PPW  europarl  ...                 False\n",
              "3IKMEYR0LWVMA6MICBSDXGSR5U2K21  europarl  ...                 False\n",
              "37M4O367VJI9ZR58F67RA0N7E885MA  europarl  ...                 False\n",
              "36JW4WBR06KF9AXMUL4N476OMH3FHC  europarl  ...                 False\n",
              "3VJ4PFXFJ37PI5MYJ4PU9LKNJ8BUAW  europarl  ...                 False\n",
              "\n",
              "[917 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR0rYPVNfW0g",
        "outputId": "5889f31f-469c-40bc-b9f3-94aa67a5adb8"
      },
      "source": [
        "stat_feature = np.array(df_Train[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\r\n",
        "\r\n",
        "embed_word = np.array(list(df_Train['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))\r\n",
        "embed_sentence = np.array(list(df_Train['sentence'].apply\r\n",
        "    (\r\n",
        "    lambda x:\r\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\r\n",
        "    )\r\n",
        "                              )\r\n",
        "                         )\r\n",
        "Train_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence))\r\n",
        "Train_Vector.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7662, 305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvNsj9g5q1KM",
        "outputId": "2ab8eff5-7b56-43c0-96d1-cdb04202c6be"
      },
      "source": [
        "stat_feature = np.array(df_Test[['token_length','token_syllable','token_vowels','token_frequency','token_contain_numeral']])\r\n",
        "\r\n",
        "embed_word = np.array(list(df_Test['token'].apply(lambda x:word_vector[x] if x in word_vector else word_vector['unk'])))\r\n",
        "embed_sentence = np.array(list(df_Test['sentence'].apply\r\n",
        "    (\r\n",
        "    lambda x:\r\n",
        "    sum([word_vector[w] if w in word_vector else word_vector['unk'] for w in x.split()])/len(x.split())\r\n",
        "    )\r\n",
        "                              )\r\n",
        "                         )\r\n",
        "Test_Vector = np.hstack((stat_feature,.5*embed_word+.5*embed_sentence))\r\n",
        "Test_Vector.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(917, 305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQEjt5cPvjcc"
      },
      "source": [
        "submission_folder = \"predictions/single_sanket\"\r\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFn6FxfGwgOo"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import gensim.downloader\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "\r\n",
        "from sklearn.svm import SVR\r\n",
        "from sklearn.neural_network import MLPRegressor\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor\r\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRFS8Mvlv_Tc"
      },
      "source": [
        "# Linear Regression\r\n",
        "reg = LinearRegression().fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/linear_regression_baseline.csv\", index=False, header=False)\r\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L30VeUnexNzQ"
      },
      "source": [
        "\r\n",
        "# SVM regressor\r\n",
        "reg = SVR().fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/SVM_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Ionk_jxf8C"
      },
      "source": [
        "\r\n",
        "# Gradient Boosting\r\n",
        "reg = GradientBoostingRegressor().fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/gradient_boosting_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcqtLvUTxrF6"
      },
      "source": [
        "\r\n",
        "# MLP Regressor\r\n",
        "regr = MLPRegressor(hidden_layer_sizes=(150)).fit(Train_Vector, np.array(df_Train['complexity']))\r\n",
        "y_pred = reg.predict(Test_Vector)\r\n",
        "\r\n",
        "pred = pd.DataFrame({\"ID\":df_Test.index, \"complexity\":y_pred})\r\n",
        "pred.to_csv(submission_folder+\"/MLP_baseline.csv\", index=False, header=False)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb3PIE5RyMO1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}